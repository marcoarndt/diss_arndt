%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Kapitel 2 - Stand der Forschung und Technik %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Stand der Forschung und Technik} \label{chap:stand}

Dieses Kapitel stellt die für diese Arbeit erforderlichen technischen und methodischen Grundlagen bereit. Zunächst werde in Abschnitt~\ref{sec:zuv} zentrale Begriffe und Konzepte der Zuverlässigkeitstechnik sowie das grundlegende statistische Verfahren zur Lebensdauer-Datenanalyse in Kombination mit Versuchsplänen erläutert.
Darauf aufbauend folgen in Abschnitt~\ref{sec:doe} die Einführung und die Einordnung von \acs{DoE} für Lebensdaueruntersuchungen sowie der multivariaten Lebensdauermodellierung aus dem Stand der Technik und der Wissenschaft, die beide für die Entwicklung effizienter Lebensdauerversuchspläne maßgeblich sind.
Im Kontext der Lebensdauererprobung umfasst dies insbesondere typische, statistische Versuchspläne sowie Metriken und Indikatoren zur allgemeinen Bewertung der Versuchspläne.

\section{Zuverlässigkeitstechnik und Wahrscheinlichkeitstheorie} \label{sec:zuv}
Die Zuverlässigkeitstechnik befasst sich mit der probabilistischen Beschreibung der Lebensdauer technischer Produkte und Systeme sowie der strategischen und statistischen Planung von Lebensdauertests.
Ziel ist die statistische Modellierung des Ausfallverhaltens unter Berücksichtigung der Funktionalität des Produkts bei relevanten Randbedingungen.
Eine zentrale Aufgabe besteht somit in der statistischen Charakterisierung des Ausfallbegriffs mithilfe deskriptiver Statistik sowie in der Parametrisierung geeigneter Verteilungen zur Abbildung des Lebensdauerverhaltens.
Die Modellierung kann - abhängig von den Randbedingungen - auf Basis \textit{einer einzelnen} Belastungsgröße oder \textit{mehrerer} Beanspruchungsparameter erfolgen, die gemeinsam den Produktausfall determinieren.
Ein grundlegendes Verständnis des Umgangs mit zufallsverteilten Lebensdauerereignissen ist daher eine elementare Voraussetzung für die statistische Versuchsplanung im Rahmen der Zuverlässigkeitstechnik.
Weiterführende Konzepte und vertiefte methodische Ansätze zur Zuverlässigkeitstechnik sowie zur statistischen Testplanung sind allen voran in der Standardliteratur von \textcite{Bertsche.2022} dargelegt, an deren Vorgehensweise sich die nachfolgenden Ausführungen orientieren.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Begriffe und Definitionen} \label{subsec:begriffezuv}
Der \textbf{Ausfall} eines technischen Produkts bezeichnet den Zeitpunkt innerhalb seiner Lebensdauer, zu dem die geforderte Funktionalität unter definierten Umgebungs- und Randbedingungen nicht mehr erfüllt ist - also das Lebensdauerende - engl. \textbf{\ac{EoL}}.
Als \textbf{Belastung} werden die von außen auf ein Produkt einwirkenden Einflussparameter - Kräfte und Momente im mechanischen Kontext - bezeichnet.
\textit{Einzelne} oder zeitgleich \textit{mehrere} Einflussparameter induzieren infolge der Produktgestalt daraus \textbf{Beanspruchungen}: innere Kräfte, Momente und lokale Spannungen.
Belastung und Beanspruchung sind die maßgeblichen Faktoren, welche die Lebensdauer determinieren.
Die \textbf{Ausfallzeit}, welche diese Zustandsänderung zeitlich definiert, wird im Allgemeinen als kontinuierliche Zufallsvariable $\sym{tau}>0$ aufgefasst.
So ergibt sich die Wahrscheinlichkeit, dass ein Produkt im Zeitraum bis $\sym{t}$ einen Funktionsverlust erleidet, zu
\begin{equation} \label{eq:probdef}
    \sym{F}(\sym{t})=\sym{Pr}(\sym{tau}\leq \sym{t}) = \int_{0}^{\sym{t}} \sym{f}(\sym{t}) \,d\sym{t}.
\end{equation}
Diese Funktion beschreibt die \textbf{Ausfallwahrscheinlichkeit}.
Sie definiert damit die Verteilungsfunktion - engl. \textbf{\ac{cdf}} - für stochastische \ac{EoL}-Events, während die \textbf{Zuverlässigkeit}
\begin{equation} \label{eq:reldef}
    \sym{R}(\sym{t})=\sym{Pr}(\sym{tau} > \sym{t}) = 1 - \sym{F}(\sym{t}) = \int_{\sym{t}}^{\infty} \sym{f}(\sym{t}) \,d\sym{t} , \quad \sym{t} \geq 0.
\end{equation}
komplementär diejenige Wahrscheinlichkeit $\sym{R}(\sym{t}): \sym{RR}_{\geq 0} \rightarrow [0,1] \subset \sym{RR}$ quantifiziert, zu der das nicht reparierbare Produkt die realisierte Zeit $\sym{t}$ überlebt: also frei von Funktionsverlust bleibt und funktionsfähig ist \cite{Bertsche.2022,Birolini.2017,Meeker.2022}.
Damit ist die Zuverlässigkeit mathematisch als reellwertige, monoton fallende und stetige Funktion definiert.
Gleichwohl ist $\sym{R}(\sym{t})$ keine universelle Eigenschaft, sondern vielmehr eine Funktion der Betriebsbedingungen.
Diese Bedingungen umfassen unter anderem eine oder mehrere Belastungsarten und deren Niveaus, Nutzungsverhalten sowie
spezifische Betriebsprofile. Mechanische, elektrische und thermische Belastungen treten dabei am häufigsten auf \cite{Yang.2007}.
Die \textbf{Wahrscheinlichkeitsdichtefunktion} - engl. \textbf{\ac{pdf}} - $\sym{f}(\sym{t})$ der Ausfallzeit beschreibt, wie sich die Wahrscheinlichkeiten der Ausfälle über der Zeit verteilen.
Sie folgt somit der Ableitung der \ac{cdf}:
\begin{equation} \label{eq:pdfdef}
    \sym{f}(\sym{t}) = \frac{d}{d\sym{t}}\sym{F}(\sym{t}) = \frac{d}{d\sym{t}}\sym{Pr}(\sym{tau} \leq \sym{t}), \quad \sym{t} \geq 0.
\end{equation}
Damit repräsentiert $\sym{f}(\sym{t})$ die Ausfallintensität pro Zeiteinheit und ist proportional zur lokalen Änderungsrate der Ausfallwahrscheinlichkeit.
Als vierte fundamentale Größe der Zuverlässigkeitsanalyse wird außerdem die \textbf{Ausfallrate} (auch Hazard-Funktion) $\sym{lambda}(\sym{t})$ eingeführt.
Sie quantifiziert das momentane Ausfallrisiko eines Produkts zum Zeitpunkt $\sym{t}$, bedingt dadurch, dass es bis zu diesem Zeitpunkt überlebt hat ($\sym{R}(\sym{t}) > 0$).
Mathematisch ist sie als das Verhältnis der \ac{pdf} zur Zuverlässigkeitsfunktion $\sym{R}(\sym{t})$ definiert:
\begin{equation} \label{eq:hazarddef}
    \sym{lambda}(\sym{t}) = \lim_{\Delta \sym{t} \to 0} \frac{\sym{Pr}(\sym{t} < \sym{tau} \leq \sym{t} + \Delta \sym{t} | \sym{tau} > \sym{t})}{\Delta \sym{t}} = \frac{1}{\sym{R}(\sym{t})} \left[ - \frac{d\sym{R}(\sym{t})}{d\sym{t}} \right] = \frac{\sym{f}(\sym{t})}{\sym{R}(\sym{t})} .
\end{equation}
Die Ausfallrate $\sym{lambda}(\sym{t})$ ist von zentraler Bedeutung, da ihr zeitlicher Verlauf (z.B. konstant, steigend, fallend) direkte Rückschlüsse auf zugrundeliegende Ausfallmechanismen wie Frühausfälle, Zufallsausfälle oder Verschleiß (vgl. "Badewannenkurve") zulässt \cite{Bertsche.2022,Yang.2007,Rigdon.2022}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deskriptive Statistik für Lebensdauerdaten} \label{subsec:stat}
Die im vorherigen Abschnitt definierten Funktionen $\sym{F}(\sym{t})$, $\sym{R}(\sym{t})$, $\sym{f}(\sym{t})$ und $\sym{lambda}(\sym{t})$ beschreiben das stochastische Ausfallverhalten eines Produktes auf einer theoretischen Populationsebene.
Für die praktische Anwendung im Engineering müssen diese Funktionen, respektive die Parameter der ihnen zugrundeliegenden Verteilungsmodelle, auf Basis von empirisch ermittelten Lebensdauerdaten jedoch approximiert werden.

Die deskriptive Statistik stellt die notwendigen Methoden zur initialen Charakterisierung, Quantifizierung und Aufbereitung dieser Stichprobendaten bereit.
Zur Beschreibung der Lebensdauerverteilungen sind \textbf{Lageparameter} und \textbf{Streuungsmaße} notwendig, die zunächst theoretisch (für die Grundgesamtheit) definiert und anschließend aus der Stichprobe berechnet werden.

Der primäre Lageparameter ist der \textbf{Erwartungswert} $\sym{mu}$ der Zufallsvariable $\sym{tau}$.
Er repräsentiert den Schwerpunkt von $\sym{f}(\sym{t})$ und wird für kontinuierliche Lebensdauerdaten berechnet als:
\begin{equation} \label{eq:theo_mean}
    \sym{mu} = \sym{E}[\sym{tau}] = \int_{0}^{\infty} \sym{t} \cdot \sym{f}(\sym{t}) d\sym{t}.
\end{equation}
Ein weiterer Lageparameter ist das \textbf{Quantil} $\symsub{t}{idx_q}$ der Lebensdauer.
Es definiert den Zeitpunkt, zu dem $\sym{F}(\sym{t})$ den Anteil $\sym{q}$ (respektive das \textbf{Perzentil} in Prozentpunkten) erreicht:
\begin{equation} \label{eq:quantildef}
    \sym{F}(\symsub{t}{idx_q}) = \sym{q}, \quad \sym{q} \in [0,1].
\end{equation}
Damit gibt das $\sym{q}$~-~Quantil denjenigen Lebensdauerwert an, unterhalb dessen der Anteil $\sym{q}$ aller betrachteten Produkte ausgefallen ist.
Ein spezieller Fall ist der \textbf{Median} $\sym{t}_{0.5}$, bei dem die Ausfallwahrscheinlichkeit 50\% beträgt:
\begin{equation} \label{eq:mediandef}
    \sym{F}(\sym{t}_{0.5}) = 0.5.
\end{equation}
Der Median beschreibt somit den Zeitpunkt, zu dem die Hälfte aller Produkte ausgefallen ist.
Damit teilt er die Fläche 1 unter der \ac{pdf} in zwei gleich große Teilflächen \cite{Yang.2007,Fahrmeir.2016}.

Das primäre Streuungsmaß ist die \textbf{theoretische Varianz} $\sym{sigma_sq}$, welche die mittlere quadratische Abweichung vom Erwartungswert beschreibt:
\begin{equation} \label{eq:theo_variance}
    \sym{sigma_sq} = \sym{Var}[\sym{tau}] = \sym{E}[(\sym{tau} - \sym{mu})^2] = \int_{0}^{\infty} (\sym{t} - \sym{mu})^2 \cdot \sym{f}(\sym{t}) d\sym{t}.
\end{equation}
Da $\sym{mu}$ und $\sym{sigma_sq}$ als theoretische Parameter üblicherweise unbekannt sind, werden auch sie durch empirische Statistiken approximiert, die aus einer Stichprobe vom Umfang $\sym{n}$ (bestehend aus den Messwerten $\sym{x}_{1}, \dots, \symsub{x}{n}$) berechnet werden.
Diese werden wiederum als Realisierungen der Zufallsvariable $\sym{tau}$ aufgefasst.

Das gängige empirische Äquivalent für den Erwartungswert $\sym{mu}$ ist der \textbf{arithmetische Mittelwert} $\sym{x_bar}$:
\begin{equation} \label{eq:emp_mean}
    \sym{x_bar} = \frac{1}{\sym{n}} \sum_{\sym{idx_i}=1}^{\sym{n}} \symsub{x}{idx_i}.
\end{equation}
Analog wird die theoretische Varianz $\sym{sigma_sq}$ durch die \textbf{empirische Varianz} $\sym{s_sq}$ (eine erwartungstreue Kenngröße) approximiert:
\begin{equation} \label{eq:emp_variance}
    \sym{s_sq} = \frac{1}{\sym{n}-1} \sum_{\sym{idx_i}=1}^{\sym{n}} (\symsub{x}{idx_i} - \sym{x_bar})^2.
\end{equation}
Die \textbf{empirische Standardabweichung} $\sym{s} = \sqrt{\sym{s_sq}}$ dient entsprechend als Näherung für die theoretische Standardabweichung $\sqrt{\sym{sigma_sq}}$.\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parametrische Lebensdauermodelle} \label{subsec:paramleben}

Während die deskriptiven Statistiken $\sym{x_bar}$ und $\sym{s_sq}$ die zentrale Tendenz und die Streuung der vorliegenden Stichprobe quantifizieren, erlauben sie keine Extrapolation oder die Modellierung der zugrundeliegenden Funktionen $\sym{F}(\sym{t})$ und $\sym{f}(\sym{t})$ der Grundgesamtheit.
Um eine prädiktive, mathematische Beschreibung des stochastischen Ausfallverhaltens zu erhalten, müssen die in Abschnitt~\ref{subsec:begriffezuv} definierten Lebensdauerfunktionen durch geeignete parametrische Verteilungsmodelle approximiert werden.
Andernfalls können nur nichtparametrische Modellierungsansätze zur Schätzung der kumulierten Wahrscheinlichkeit in Überlebensfunktionen wie beispielsweise nach \textcite{Kaplan.1958} genutzt werden \cite{Rigdon.2022,Meeker.2022}.
Die Verteilungsmodelle hingegen bieten eine geschlossene mathematische Form für \ac{cdf} und \ac{pdf} und ermöglichen es, das komplexe Ausfallverhalten durch eine geringe Anzahl von Parametern zu charakterisieren.\

\subsubsection{Weibull-Verteilung} \label{subsubsec:weibull}
In der Zuverlässigkeitstechnik hat sich die \textbf{Weibull-Verteilung} aufgrund ihrer hohen Flexibilität als das am häufigsten verwendete Modell etabliert.
Je nach zugrundeliegendem physikalischen Ausfallmechanismus finden jedoch auch andere statistische Verteilungen Anwendung, wie beispielsweise die \textbf{Lognormal-Verteilung} (häufig bei Ermüdungs-, Korrosions- oder Diffusionsprozessen), die \textbf{Exponentialverteilung} (zur Modellierung von Zufallsausfällen ohne Alterungseffekte) oder die \textbf{Beta-Verteilung} (allgemein zur formenreichen Modellierung von $\sym{R}$ über dem festen Intervall $[0,1]$).
Für weitere Ausführungen dazu sei an dieser Stelle jedoch auf bereits ausreichend diskutierte Aufbereitungen von \textcite{Bertsche.2022,Birolini.2017,Yang.2007,Hedderich.2020,Rigdon.2022} verwiesen.

Die (zweiparametrige) Weibull-Verteilung ist das Standardmodell zur Beschreibung der Lebensdauer von technischen Produkten ohne die Berücksichtigung eines möglichen dritten Parameters - der ausfallfreien Zeit \symsub{t}{idx_0}.
Sie wird durch den \textbf{Formparameter}  $\sym{b} > 0$ (Weibull-Modul) und die \textbf{charakteristische Lebensdauer} $\sym{T} > 0$ (Skalenparameter), welche dem $63,2$-ten Perzentil $\sym{t}_{0,632}$ entspricht, beschrieben.
Unabhängig von $\sym{b}$ gilt hier: $\sym{F}(\sym{T})=1-e^{-1} \approx 63,2 \%$.
Folgt die Lebensdauer-Zufallsvariable $\sym{tau}$ dieser Verteilung, wird dies mathematisch als $\sym{tau} \sim \sym{W}(\sym{T}, \sym{b})$ notiert.
Damit ist sie in der Lage, alle drei Phasen der "Badewannenkurve" (Frühausfälle mit $\sym{b}<1$, Zufallsausfälle $\sym{b}\approx 1$, Verschleißausfälle mit $\sym{b}>1$) durch die Wahl ihrer Parametrisierung abzubilden, vgl. \textcite{Bertsche.2022}.

Die Einheit des Skalenparameters $\sym{T}$ entspricht der Einheit des Messwertes ($\sym{t}$ in Stunden, Überrollungen, Kilometer, etc.).
Die \ac{pdf} der Weibull-Verteilung ist damit definiert als:
\begin{equation} \label{eq:weibull_pdf}
    \sym{f}(\sym{t}) = \frac{\sym{b}}{\sym{T}^{\sym{b}}} \sym{t}^{\sym{b}-1} \exp\left[ - \left(\frac{\sym{t}}{\sym{T}}\right)^{\sym{b}} \right], \quad \sym{t} > 0.
\end{equation}
Die \ac{cdf} ergibt sich durch Integration der \ac{pdf} zu:
\begin{equation} \label{eq:weibull_cdf}
    \sym{F}(\sym{t}) = 1 - \exp\left[ - \left(\frac{\sym{t}}{\sym{T}}\right)^{\sym{b}} \right], \quad \sym{t} > 0.
\end{equation}
Aus $\sym{f}(\sym{t})$ und $\sym{R}(\sym{t}) = 1 - \sym{F}(\sym{t})$ leitet sich die \textbf{Ausfallrate} $\sym{lambda}(\sym{t})$ der Weibull-Verteilung ab:
\begin{equation} \label{eq:weibull_hazard}
    \sym{lambda}(\sym{t}) = \frac{\sym{b}}{\sym{T}} \left(\frac{\sym{t}}{\sym{T}}\right)^{\sym{b}-1}, \quad \sym{t} > 0.
\end{equation}
Der Erwartungswert $\sym{mu}$ (vgl. Gleichung~\eqref{eq:theo_mean}) und die Varianz $\sym{sigma_sq}$ (vgl. Gleichung~\eqref{eq:theo_variance}) der Weibull-Verteilung lassen sich ebenfalls in geschlossener Form ausdrücken. Sie sind von der \textbf{Gamma-Funktion} $\sym{Gamma}(\cdot)$ abhängig, welche für $x > 0$ definiert ist als:
\begin{equation} \label{eq:gamma_func}
    \sym{Gamma}(x) = \int_{0}^{\infty} \sym{ups}^{x-1} \exp(-\sym{ups}) \,d\sym{ups}.
\end{equation}
Der Erwartungswert $\sym{mu}$ der Weibull-verteilten Lebensdauer $\sym{tau}$ ergibt sich zu:
\begin{equation} \label{eq:weibull_mean}
    \sym{mu} = \sym{E}[\sym{tau}] = \sym{T} \cdot \sym{Gamma}\left(1 + \frac{1}{\sym{b}}\right).
\end{equation}
Die Varianz $\sym{sigma_sq}$ ist gegeben durch:
\begin{equation} \label{eq:weibull_variance}
    \sym{sigma_sq} = \sym{Var}[\sym{tau}] = \sym{T}^2 \left[ \sym{Gamma}\left(1 + \frac{2}{\sym{b}}\right) - \sym{Gamma}^2\left(1 + \frac{1}{\sym{b}}\right) \right].
\end{equation}

Die Ausprägung der erwähnten Flexibilität der Weibull-Verteilung ist durch Abbildung~\ref{fig:abb2.1_weibull} nachvollziehbar.
Ein Spezialfall tritt ein für $\sym{b}=1$, da sich in diesem Fall die Weibull-Verteilung zur Exponentialverteilung mit dem Ausfallraten-Parameter $\sym{lambda} = 1/\sym{T}$ und dem Erwartungswert $\sym{mu} = \sym{T}$ reduziert.
Für $\sym{b}=3,6$ wird die Schiefe der \ac{pdf} annähernd eliminiert, sodass sich die Weibull-Verteilung einer Normalverteilung annähert \cite{Rinne.2008, Kececioglu.2002}.

\begin{figure}[htbp]
    \centering
    \input{plots/ma_abb2.01_weibull}
    \caption{Weibull $\sym{f}(\sym{t})$ für ausgewählte Werte von $\sym{T}$ und $\sym{b}$. }
    \label{fig:abb2.1_weibull}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parameterschätzverfahren} \label{subsec:schätzer}
Soll eine geschlossene mathematische Beschreibung des stochastischen Ausfallverhaltens eines Produktes gefunden werden, ist das im vorherigen Abschnitten~\ref{subsec:stat} definierte parametrische Verteilungsmodell $\sym{tau} \sim \sym{W}(\sym{T}, \sym{b})$ zu schätzen.
Die Modellparameter der Grundgesamtheit sind in der praktischen Anwendung jedoch unbekannt.
Die zentrale Problemstellung der \textbf{Parameterschätzung} besteht somit darin, aus der empirischen Stichprobe bestehend aus $\sym{n}$ Realisierungen $\sym{t}_{1}, \dots, \symsub{t}{n}$ der Zufallsvariable $\sym{tau}$ statistisch fundierte Schätzwerte $\hat{\sym{T}}$ und $\hat{\sym{b}}$ zu gewinnen.
Diese sind Voraussetzung, um das Lebensdauermodell (z.B. Gleichung~\eqref{eq:weibull_cdf}) zu quantifizieren und prädiktive Aussagen zu Quantilen oder der Zuverlässigkeit $\sym{R}(\sym{t})$ zu ermöglichen.
Eine wesentliche Komplikation hierbei sind jedoch das mögliche Auftreten von unvollständigen bzw. \textbf{zensierten} Daten sowie \textit{multivariate} Abhängigkeiten der Belastungen zur Messgröße.
Während für die Schätzung von Verteilungsparametern einfache Verfahren, wie die \textbf{Momentenmethode} oder die \textbf{Methode der kleinsten Fehlerquadrate} - engl. \ac{OLS}, die beispielsweise bei der \ac{MMR} im Wahrscheinlichkeitsnetz Anwendung findet, existieren, sind diese für die umfassende Analyse vielschichtiger Lebensdauerdaten in der Regel unzureichend und hier nur der Vollständigkeit wegen erwähnt - vgl. \cite{Bertsche.2022,Montgomery.2021}.
Das universell anwendbare und robuste Verfahren, das Herausforderungen wie zensierte Daten und multivariate Modelle inhärent behandelt, ist die \ac{MLE} \cite{Meeker.2022,Nelson.1990}.

\subsubsection{Maximum-Likelihood-Estimation} \label{subsubsec:mle}
Das Grundprinzip der \ac{MLE} besteht darin, diejenigen Parameterwerte (z.B. $\hat{\sym{T}}, \hat{\sym{b}}$) als Schätzwerte auszuwählen, welche die Wahrscheinlichkeit (engl. Likelihood) maximieren, die empirisch beobachtete Stichprobe (bestehend aus unabhängigen Ausfällen und Zensierungen) zu erhalten.
Mathematisch wird die Wahrscheinlichkeit der Realisierung von $\sym{t_vec} = (\sym{t}_{1}, \dots, \symsub{t}{n})$ einer Stichprobe durch die \textbf{Likelihood-Funktion} $\sym{L_like}$ bestimmt. Diese ist eine Funktion des unbekannten Parametervektors $\sym{theta}$, der $\sym{k}$ zu schätzende Parameter enthält (z.B. $\sym{theta} = (\sym{T}, \sym{b})$ mit $\sym{k}=2$).\

Für den vereinfachten Fall, dass die Stichprobe ausschließlich aus $\sym{n}$ exakten Ausfallereignissen (vollständige Daten) besteht, ist die Likelihood-Funktion $\sym{L_like}$ das Produkt der einzelnen Wahrscheinlichkeitsdichten $\sym{f}(\cdot)$:
\begin{equation} \label{eq:mle_likelihood_simple}
    \sym{L_like}(\sym{t_vec} | \sym{theta}) \propto \prod_{\sym{idx_i}=1}^{\sym{n}} \sym{f}(\symsub{t}{idx_i} | \sym{theta}).
\end{equation}
Zur Vereinfachung der numerischen Berechnung wird in der Anwendung die \textbf{Log-Likelihood-Funktion} $\sym{Lambda}$ verwendet. Durch die Logarithmierung wird das Produkt (Gleichung~\eqref{eq:mle_likelihood_simple}) in eine äquivalente, leichter zu maximierende Summe überführt:
\begin{equation} \label{eq:mle_loglikelihood_simple}
    \sym{Lambda} \defeq \ln\left( \sym{L_like}(\sym{theta}) \right) \propto \sum_{\sym{idx_i}=1}^{\sym{n}} \ln \left[ \sym{f}(\symsub{t}{idx_i} | \sym{theta}) \right].
\end{equation}
Wie zuvor dargelegt, ist dieser vereinfachte Ansatz für Lebensdauerdaten jedoch oft unzureichend, da er das Auftreten von zensierten Daten vernachlässigt.
Für die praktische Anwendung existiert jedoch die entsprechende Erweiterung der Likelihood-Funktion um die Differenzierung etwaiger Testausgänge als \textit{Durchläufer}.
Dazu wird die Stichprobe als Paarung von $\symsub{t}{idx_i}, \symsub{delta}{idx_i}$ für $\sym{t_vec}$ definiert, wobei $\symsub{t}{idx_i}$ der beobachteten Zeit und $\symsub{delta}{idx_i}$ einem Statusindikator ($\symsub{delta}{idx_i}=1$ für einen exakten Ausfall; $\symsub{delta}{idx_i}=0$ für eine Rechts-Zensierung) entspricht \cite{Meeker.2022,Kalbfleisch.2002}.
$\sym{L_like}$ für rechts-zensierte Lebensdauerdaten lautet somit:
\begin{equation} \label{eq:mle_likelihood_censored}
    \sym{L_like}(\sym{t_vec} | \sym{theta}) \propto \prod_{\sym{idx_i}=1}^{\sym{n}} \left[ \sym{f}(\symsub{t}{idx_i} | \sym{theta})^{\symsub{delta}{idx_i}} \cdot \sym{R}(\symsub{t}{idx_i} | \sym{theta})^{1 - \symsub{delta}{idx_i}} \right]
\end{equation}
und definiert die Log-Likelihood Funktion als:
\begin{equation} \label{eq:mle_loglikelihood_censored}
    \sym{Lambda} \defeq \ln\left(\sym{L_like}(\sym{t_vec} | \sym{theta})\right) \propto \sum_{\sym{idx_i}=1}^{\sym{n}} \left[ \symsub{delta}{idx_i} \cdot \ln \sym{f}(\symsub{t}{idx_i} | \sym{theta}) + (1 - \symsub{delta}{idx_i}) \cdot \ln \sym{R}(\symsub{t}{idx_i} | \sym{theta}) \right].
\end{equation}

Der Parametervektor $\hat{\sym{theta}}$, der den Wert von $\sym{Lambda}(\sym{theta})$ maximiert, liefert die \ac{MLE}-Werte.
Die Schätzwerte repräsentieren die (asymptotisch) effizientesten Schätzwerte für die Parameter der Grundgesamtheit.
Dies erfolgt mathematisch durch Nullsetzen $\sym{k}$ partieller Ableitungen von $\sym{Lambda}$, sofern mathematisch entsprechende Schätzwerte in geschlossener Form durch $\partial\sym{Lambda}/\partial \sym{theta} \stackrel{!}{=} 0$ identifiziert werden können \cite{Nelson.2005,Rinne.2008}.
Andernfalls werden numerische Optimierungsalgorithmen, vgl. Newton-Raphson-Verfahren, Patternsearch und vergleichbare, dafür herangezogen - siehe weiterführend \cite{Nelson.2005,Qiao.1994} sowie detaillierte Untersuchungen von \textcite{Kremer.2019b}.
An dieser Stelle sei erwähnt, dass systematische Verzerrungen (engl. \textbf{Bias}) in $\hat{\sym{theta}}$ aufgrund kleiner Stichprobenumfänge auftreten können \cite{Abernethy.2006} - jedoch auch korrigierbar sind, vgl. Arbeiten von \textcite{Hirose.1999,Ross.1996}.

Die Qualität der Parameterschätzung beeinflusst daraus nicht nur die Prädiktionsgüte zur Schätzung der Lebensdauer oder Zuverlässigkeit - sie bedingt schließlich auch die Effizienz des Schätzverfahrens.
Wird im Sinne eines effizienten Verfahrens zur multivariaten Lebensdauermodellbildung eine Methodik gesucht, ist auch die Qualität der Parameterschätzung damit entscheidend.
Vertrauensbereiche, oder engl. \acp{CI}, können eine Metrik für die Qualität der Modellierung einnehmen, da sie die Unsicherheit oder \textit{Unschärfe} in der Prädiktion bemessen.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\subsubsection{Vertrauensbereiche} \label{subsubsec:ci}
Die \ac{MLE} liefert nicht nur die Punktschätzer $\hat{\sym{theta}}$, sondern auch die Quantifizierung von deren statistischer Unsicherheit (Präzision).
Obwohl verschiedene Ansätze, wie die numerisch anspruchsvolleren Berechnungen nach Likelihood-Ratio-Methode, Bootstrap-Perzentil-Methode oder Monte-Carlo-Approximation existieren, ist das gängigste Verfahren zur Berechnung von \acp{CI} die Approximation mittels asymptotischer Normalverteilung der \ac{MLE}-Schätzer $\hat{\sym{theta}}$ \cite{Bertsche.2022,Nelson.1990}.
Dies erfolgt über die \textbf{Fisher-Informationsmatrix} $\sym{FIM}$, welche die Information der Stichprobe über die Parameter $\sym{theta}$ gemäß \textcite{Kremer.2019c}  bezüglich des Rechenaufwands und resultierender Modellqualität vergleichsweise effizient quantifiziert.
So wird diese Methodik auch in gängiger Applikationen als Standard angewandt, vgl. \cite{Nelson.2005,Nelson.1990,Yang.2007}.
In der praktischen Anwendung wird die Fisher-Informationsmatrix auf Basis der resultierenden Schätzwerte $\hat{\sym{theta}} = \sym{theta}$ so als Schätzung zur Beobachtung nach $\symsub{FIM}{idx_O}$ verwendet \cite{Nelson.1990,Lawless.2003}.
Diese ist definiert als die negative \textbf{Hesse-Matrix} $\sym{H}$ der Log-Likelihood-Funktion, ausgewertet an der Stelle der \ac{MLE}-Schätzwerte
$\hat{\sym{theta}}$:
\begin{equation} \label{eq:fim_o}
    \symsub{FIM}{idx_O} \defeq - \sym{H}(\hat{\sym{theta}}) = - \left[ \frac{\partial^2 \sym{Lambda}(\sym{theta})}{\partial \symsub{theta}{idx_j} \partial \symsub{theta}{idx_l}} \right]_{\sym{theta} = \hat{\sym{theta}}}.
\end{equation}
Die Matrix $\sym{H}$ entspricht der ($\sym{k} \times \sym{k}$)-Matrix der $\sym{k}$ zweiten partiellen Ableitungen von $\sym{Lambda}$ (vgl. Gl.~\eqref{eq:mle_loglikelihood_censored}).
Eine Invertierung ${\hat{\sym{FIM}}_{\sym{idx_O}}}^{-1}$ ergibt die geschätzte \textbf{Varianz-Kovarianz-Matrix} $\hat{\sym{V}}$:
\begin{equation} \label{eq:var_covar}
    \hat{\sym{V}} \approx {\hat{\sym{FIM}}_{\sym{idx_O}}}^{-1}.
\end{equation}
Die Diagonalelemente dieser Matrix $\hat{\sym{V}}_{\sym{idx_j}\sym{idx_j}}$ entsprechen den Varianzen $\sym{Var}(\hat{\sym{theta}}_{\sym{idx_j}})$ der einzelnen Parameterschätzwerte \cite{Nelson.1990}.
Die Nicht-Diagonalelemente $\hat{\sym{V}}_{\sym{idx_j}\sym{idx_l}}$ (für $\sym{idx_j} \neq \sym{idx_l}$) repräsentieren die \textbf{Kovarianzen} $\sym{Cov}(\hat{\sym{theta}}_{\sym{idx_j}}, \hat{\sym{theta}}_{\sym{idx_l}})$ \cite{Nelson.1990,Yang.2007}.
Diese Kovarianzen sind von entscheidender Bedeutung, da sie die statistische Abhängigkeit zwischen den Schätzwerten (z.B. zwischen $\hat{\sym{T}}$ und $\hat{\sym{b}}$) quantifizieren, welche für die Berechnung der \acp{CI} von abgeleiteten Funktionen wie $\hat{\sym{R}}(\sym{t})$ erforderlich sind \cite{Meeker.2022}.
Basierend auf der Annahme der asymptotischen Normalität der Schätzer wird ein zweiseitiges $(1-\sym{alpha})$-\ac{CI} für einen einzelnen Parameter $\hat{\sym{theta}}_{\sym{idx_j}}$ direkt aus dessen Varianz approximiert durch:
\begin{equation} \label{eq:ci_normal}
    [\sym{theta}_{\sym{idx_j},\sym{idx_u}},\sym{theta}_{\sym{idx_j},\sym{idx_o}}]  = \hat{\sym{theta}}_{\sym{idx_j}} \pm \sym{z}_{1-\sym{alpha}/2} \cdot \sqrt{\hat{\sym{V}}_{\sym{idx_j}\sym{idx_j}}},
\end{equation}
wobei $\sym{z}_{1-\sym{alpha}/2}$ dem $(1-\sym{alpha}/2)$-Quantil der Standardnormalverteilung entspricht.
Da Lebensdauerparameter (z.B. $\sym{T}, \sym{b}$) üblicherweise auf $\sym{RR}_{>0}$ beschränkt sind, werden \acp{CI} robust über eine Log-Transformation der Parameter berechnet, um physikalisch unmögliche (negative) Intervallgrenzen zu vermeiden \cite{Meeker.2022,Yang.2007,Nelson.1990}:
\begin{equation} \label{eq:ci_ln}
    [\sym{theta}_{\sym{idx_j},\sym{idx_u}}, \sym{theta}_{\sym{idx_j},\sym{idx_o}}] = \hat{\sym{theta}}_{\sym{idx_j}} \exp \left( \pm \sym{z}_{1-\sym{alpha}/2} \cdot \frac{\sqrt{\hat{\sym{V}}_{\sym{idx_j}\sym{idx_j}}}}{\hat{\sym{theta}}_{\sym{idx_j}}} \right).
\end{equation}

Die Berechnung dieser \acp{CI} für Schätzwerte $\sym{g_func}(\hat{\sym{theta}})$ zu Größen wie $\hat{\sym{R}}(\sym{t})$ oder $\hat{\sym{t}}_{\sym{idx_q}}$  erfolgt mittels \textbf{Delta-Methode} \cite{Nelson.1990,Meeker.2022}.
Dieses auf einer Taylor-Reihenentwicklung basierende Verfahren (Gauß'sche Fehlerfortpflanzung) approximiert die Varianz der Funktion $\hat{\sym{g_func}}=\sym{g_func}(\hat{\sym{theta}})$ unter Einbeziehung der gesamten Varianz-Kovarianz-Matrix.
Dazu wird der \textbf{Gradientenvektor} $\sym{g_grad}$ der Funktion $\sym{g_func}$ (z.B. $\sym{g_func} = \sym{R}(\sym{t})$) bezüglich des $\sym{k}$-dimensionalen Parametervektors $\sym{theta}$ gebildet:
\begin{equation} \label{eq:delta_method_gradient}
    \sym{g_grad} \defeq \left[ \frac{\partial \sym{g_func}(\sym{theta})}{\partial \sym{theta}_{1}}, \dots, \frac{\partial \sym{g_func}(\sym{theta})}{\partial \symsub{theta}{k}} \right]^T_{\sym{theta} = \hat{\sym{theta}}}.
\end{equation}
Die approximierte Varianz $\sym{Var}(\hat{\sym{g_func}})$ der Funktion ergibt sich dann aus:
\begin{equation} \label{eq:delta_method_variance}
    \sym{Var}(\hat{\sym{g_func}}) \approx \sym{g_grad}^T \hat{\sym{V}} \sym{g_grad}.
\end{equation}
Das Vertrauensintervall für die Funktion $\hat{\sym{g_func}}$ wird anschließend unter Verwendung dieser Varianz (bzw. des Standardfehlers $\sqrt{\sym{Var}(\hat{\sym{g_func}})}$) analog zu Gleichung~\eqref{eq:ci_normal} berechnet \cite{Bain.2017b,Yang.2007,Nelson.1990}:
\begin{equation} \label{eq:ci_rel}
    [\sym{g_func}_{\sym{idx_u}},\sym{g_func}_{\sym{idx_o}}]  = \hat{\sym{g_func}} \pm \sym{z}_{1-\sym{alpha}/2} \sqrt{\sym{Var}(\hat{\sym{g_func}})}.
\end{equation}
Sollen auch hier nur positive Werte für $\sym{g_func}$ berücksichtigt werden, kann eine Logarithmierung in der Berechnung der Vertrauensbereiche analog zu Gl.~\ref{eq:ci_ln} erfolgen \cite{Yang.2007}.

\section{Statistische Versuchsplanung und Modellbildung} \label{sec:doe}
Multivariate Lebensdauertests erfordern definitionsgemäß die Betrachtung mehrerer $\sym{k}\geq 2$ Einflussfaktoren als Versuchsparameter.
Dementsprechend entscheidend ist das Verständnis der wesentlichen Grundlagen im Umgang mit statistischer Versuchsplanung (\ac{DoE}) für Lebensdauerdaten - auch unter dem Begriff \textbf{\ac{L-DoE}} zusammengefasst - sowie der darauffolgenden Lebensdauermodellbildung.
Während detaillierte Übersichten zur Historie von \ac{DoE} von anfänglichen einschlägigen Beschreibungen durch \textcite{Fisher.1935}, außerdem maßgebliche Weiterentwicklungen durch \textcite{Box.1978} oder evolutionäre Schritte durch \textcite{Taguchi.2005} ausgiebig in Werken von \textcite{Kleppmann.2020,Rigdon.2022,Montgomery.2020} beschrieben sind, wird im Folgenden auf die wesentlichen Inhalte für die Forschungsschwerpunkte eingegangen.

Der primäre Anspruch von \ac{DoE} besteht in der effizienten Planung empirischer Datenerhebungen, um Zielgrößen in Abhängigkeit erklärender Variablen zunächst robust zu modellieren und schließlich zu optimieren.
Dieses Paradigma lässt sich auch unter der Begrifflichkeit \ac{DfR} unmittelbar wiedererkennen und so auf die Analyse von Lebensdauer und Zuverlässigkeit übertragen \cite{Yang.2007,Wu.2021}.
Da das lokale oder globale Optimum der Lebensdauer- bzw. Zuverlässigkeitsfunktion eines Produktes a priori meist unbekannt ist, erfordert dessen Identifikation eine systematische Exploration des Parameterraumes.
Eine besondere Herausforderung stellt hierbei zusätzlich die Integration von \ac{ALT} dar: Die Diskrepanz zwischen dem hochbelasteten Testraum (engl. \textbf{Design Space}) und dem regulären \textbf{Prädiktionsraum} (engl. Use Space oder \textbf{Field Space}) kann eine \textbf{Extrapolation} erforderlich machen, welche die Anforderungen an die Daten- und somit auch an die Designqualität deutlich verschärft.
Für umfassendere Ausführungen zu Forschungserkenntnissen in \ac{ALT} sei hierbei insbesondere auf Arbeiten von \textcite{Meeker.1993,Meeker.2022,Nelson.1990, Elsayed.2007} verwiesen.
Das Unwissen zur tatsächlichen Lage optimaler Antwortwerte und die Möglichkeit, per se einen systematischen Offset zwischen Design- und Field-Space durch \ac{ALT} vorzufinden, stellen Teststrategien nach Best-Guess Ansätzen nachteilig.
Hier wird in der industriellen Praxis häufig fälschlicherweise ein \ac{OFAT}-Testing Ansatz gewählt - unabhängig, ob vom Vorhandensein von Lebensdauer- oder anderen Daten, welcher schlichtweg die Wahrscheinlichkeit, Optimalstellen im Parameterraum systematisch zu treffen, senkt und somit gegenüber \ac{L-DoE} nachteilig ist, vgl. \cite{Montgomery.2020,Siebertz.2017}.
Da nun die geometrische Struktur eines Versuchsplans die erreichbare Modellierungsqualität deterministisch begrenzt, ist eine präzise Bewertung der Plangüte anhand genau dieser Eigenschaft im Vorfeld unerlässlich.
Hierfür können objektive \textbf{Performance-Indikatoren} sowie mathematische \textbf{Optimalitätskriterien} dienen, vgl. \cite{Montgomery.2020,Goos.2011}.
Ergänzend zu rationalen Metriken wie der statistischen \textbf{Trennschärfe} (engl. \textbf{Power}) und dem Schätzergebnis einer \textbf{Koeffizienten}- bzw. Parameterschätzung sind diese Größen damit bestimmend für effiziente multivariate Lebensdauertests.

Vor diesem Hintergrund fokussiert sich dieser Abschnitt auf eine gezielte Auswahl an Grundbegriffen und Metriken für multivariate Testpläne im Kontext von \ac{L-DoE} sowie auf eine Übersicht der für Lebensdauertests geeigneten Versuchspläne, bevor abschließend die statistische Modellbildung beleuchtet wird.

Für eine grundsätzlichere Auseinandersetzung mit konventionellen Methoden und Werkzeugen von \ac{DoE} sei, mit Blick auf den Fokus der vorliegenden Arbeit, hingegen auf die einschlägige Literatur von \textcite{Kleppmann.2020}, \textcite{Siebertz.2017}, \textcite{Hinkelmann.2012} sowie vornehmlich \textcite{Montgomery.2020} und \textcite{Myers.2016} verwiesen.
Diese Werke behandeln intensiv die Inhalte grundsätzlicher statistischer Versuchsplanung, welche um Perspektiven zu \ac{L-DoE} bereits durch am \ac{IMA} entstandene Dissertationen von \textcite{Dazer.2019}, \textcite{Herzig.2021}, \textcite{Grundler.2024} und maßgeblich durch \textcite{Kremer.2021} fortschreitend ergänzt wurden.
Konsequenterweise werden Hintergründe zum Umgang mit normalverteilten Daten oder Abweichungen davon im Rahmen des \ac{DoE}, die Diskussion zu einschlägigen Vor- und Nachteilen auch unter Abgrenzung zu Alternativen wie \ac{OFAT}, die Regressionsmodellierung auf Basis der \textbf{\ac{ANOVA}}, konventionelle Hypothesentests sowie fundamentale Ausführungen zu \ac{ALT} in den nachfolgenden Ausführungen nicht explizit betrachtet, sondern als bekannt vorausgesetzt.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Grundlagen zur statistischen Versuchsplanung} \label{subsec:begriffedoe}
Die Anwendung von \ac{DoE} versteht sich grundsätzlich als Verfahrenskette entlang mehrerer Prozessschritte  \cite{Coleman.1993,Montgomery.2020}, die beginnend von einer spezifischen Aufgabendefinition in einer statistisch abgesicherten Testentscheidung und Datenmodellierung mündet, vgl. Abbildung~\ref{fig:ma_abb2.02_doe_steps}.
\begin{figure}[htbp]
    \centering
    \def\svgwidth{0.9\textwidth}
    \input{plots/ma_abb2.02_doe_steps.pdf_tex}
    \caption{\ac{DoE} Steps gemäß \cite{Coleman.1993}}
    \label{fig:ma_abb2.02_doe_steps}
\end{figure}
Das erklärte Ziel ist es, den kausalen Zusammenhang zwischen Einflussfaktoren und Systemantwort funktional abzubilden.
Darin abgebildete Einflussfaktoren sollen also per se \textbf{statistisch signifikant} und somit relevant für das Systemverhalten sein.
So kann beispielsweise die zufallsverteilte Lebensdauer $\sym{tau}$ in Abhängigkeit von $\sym{k} \geq 2$ technischen Beanspruchungen zunächst empirisch untersucht und anschließend modelliert sowie optimiert werden (\textit{Schritt~1} in Abbildung~\ref{fig:ma_abb2.02_doe_steps}).

Im Zentrum der Betrachtung steht damit generell ein technisches \textbf{System}, welches abstrakt als Produkt oder Prozess verstanden wird und den Zustand der Ausgangsgröße in Abhängigkeit der \textbf definiert.
Die zu untersuchende oder zu optimierende Ausgangsgröße wird als \textbf{Systemantwort} $\sym{y}$ (engl. \textbf{Response}) bezeichnet.
Die gezielt kontrollierbaren und variierten Eingangsgrößen sind \textbf{Faktoren} (\textbf{Steuergrößen}), während nicht kontrollierbare oder unbekannte Einflüsse als \textbf{Störgrößen} (engl. \textbf{Noise}) klassifiziert werden (\textit{Schritt~2}, vgl. \cite{Kleppmann.2020}).
Eine visuelle Aufstellung des genannten Zusammenspiels der Parameter kann dem Parameterdiagramm, kurz \textbf{P-Diagramm}, in Abbildung~\ref{fig:ma_abb2.03_p_diagramm} entnommen werden \cite{Montgomery.2020}.
\begin{figure}[htbp]
    \centering
    \def\svgwidth{0.8\textwidth}
    \input{plots/ma_abb2.03_p_diagramm.pdf_tex}
    \caption{Parameter-Diagramm (P-Diagramm)}
    \label{fig:ma_abb2.03_p_diagramm}
\end{figure}
Um das Systemverhalten zu charakterisieren, werden die Faktoren als kategoriale oder kontinuierliche Parameter im Versuch auf diskreten Werten, den sogenannten \textbf{Faktorstufen} (engl. \textbf{Level}), variiert (\textit{Schritt~3}).
Dies erfolgt in aller Regel in kodierter Darstellung, so entsprechen gemäß der gängigsten Konvention die Stufe \textit{-1} der niedrigen und \textit{+1} der hohen Einstellstufe.
Die planerische Kombination verschiedener Faktorstufen äußert sich in spezifischen \textbf{Versuchspunkten} innerhalb des Parameterraums und entspricht der Versuchsplan-Matrix \cite{Kleppmann.2020,Siebertz.2017}.
Der \textbf{Versuchsraum} (engl. \textbf{Design Space}) wird hierbei durch die Gesamtheit der technisch realisierbaren und im Versuch einstellbaren Parameterkombinationen aufgespannt.
Die Auswahl geeigneter statistischer Versuchspläne (\textit{Schritt~4}) für die Durchführung (\textit{Schritt~5}) wird in Abschnitt~\ref{subsec:pläne} detailliert behandelt.

Die aus der Variation resultierende Änderung der Systemantwort quantifiziert den Einfluss des Faktors, der statistisch als \textbf{Effekt} $\sym{Eff}$ bezeichnet wird und den Mittelwertunterschiede zweier Faktorstufen beschreibt (\textit{Schritte~6-7}).
Mittels \textbf{Kontrastmethode} wird also die Änderung der Systemantwort über alle durchgeführten Versuche mit jeweiligen Faktorstufen registriert \cite{Kleppmann.2020,Montgomery.2020}:
\begin{equation} \label{eq:eff}
    \sym{Eff}=\frac{1}{\sym{n}|_{+1}}\sum{\sym{y}|_{+1}} - \frac{1}{\sym{n}|_{-1}}\sum{\sym{y}|_{-1}}
\end{equation}
So können mittels \ac{DoE} strukturiert, effizient und verbindlich Informationen gewonnen werden, die über die direkten Effekte hinausgehen und differenziert Aufschluss über \textbf{Haupteffekte} sowie etwaige \textbf{Wechselwirkungen} der Faktoren auf die Antwort des Systems geben - vergleiche Abb.~\ref{fig:ma_abb2.04_effekt} sowie \textcite{Montgomery.2020,Kleppmann.2020,Siebertz.2017,Kremer.2021}.
Abbildung~\ref{fig:ma_abb2.04_effekt} visualisiert derartige Effekte.
So gibt die Darstellung eines Haupteffekts in Abhängigkeit des Vorzeichens und der Steigung (\textbf{positiver} oder \textbf{negativer} Haupteffekt) sozusagen die Einflussstärke und -richtung wieder, während bei Wechselwirkung der Effekt in Abhängigkeit der Einstellung eines \textbf{Co-Faktors} dargestellt wird.
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \setlength\figurewidth{0.9\linewidth}
        \setlength\figureheight{4cm}
        \input{plots/ma_abb2.04.1_effekt.tex}
        \caption{Positiver Haupteffekt $\sym{Eff}_1$}
        \label{fig:ma_abb2.04.1_effekt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \setlength\figurewidth{0.9\linewidth}
        \setlength\figureheight{4cm}
        \input{plots/ma_abb2.04.2_effekt.tex}
        \caption{Wechselwirkungseffekte $\sym{Eff}_2$ und $\sym{Eff}_3$}
        \label{fig:ma_abb2.04.2_effekt}
    \end{subfigure}
    \caption{Schematische Darstellung der Effekte: (a) positiver Haupteffekt von Faktor $\sym{x}_1$, (b) Wechselwirkungseffekte zwischen $\sym{x}_1$ und $\sym{x}_2$.}
    \label{fig:ma_abb2.04_effekt}
\end{figure}
Diese Zusammenhänge können mathematisch positiv oder negativ beschrieben sowie durch Polynomfunktionen höherer Ordnung approximiert werden, um zusätzlich beispielsweise \textbf{quadratische Effekte} oder \textbf{Mehrfachwechselwirkungen} abzubilden.
In einem einfachen Fall wird für die lineare Beschreibung des Einflusses von Faktoren auf eine Antwortvariable ein durchschnittlicher Effekt durch eine Regressionskonstante $\symsub{beta}{idx_0}$ sowie Haupteffekte durch die Regressionskoeffizienten $\symsub{beta}{idx_j}, \sym{idx_j}=0,1,\dots,\sym{k}$ zu einem Regressionsmodell erster Ordnung geschätzt.
Falls relevant, erfolgt die Ergänzung um die jeweilige Wechselwirkung und einen Fehlerterm $\sym{epsilon}$ als Zufallsvariable für Abweichungen durch Mess- und Streufehler \cite{Montgomery.2020,Myers.2016,Rigdon.2022}.
Beispielhaft $\sym{k}=2$ Faktoren resultiert daraus:
\begin{equation} \label{eq:linear_std_model}
    \sym{y}=\symsub{beta}{idx_0}+\sym{beta}_{1}\sym{x}_1+\sym{beta}_{2}\sym{x}_2+\sym{beta}_{12}\sym{x}_1\sym{x}_2+\sym{epsilon}.
\end{equation}
Ein Modell zweiter Ordnung enthält zudem quadratische Terme, welche üblicherweise in Optimierungsaufgaben - so auch in der Lebensdauer- und Zuverlässigkeitsanalyse - relevant werden können:
\begin{equation} \label{eq:quadric_std_model}
    \sym{y}=\symsub{beta}{idx_0}+\sym{beta}_{1}\sym{x}_1+\sym{beta}_{2}\sym{x}_2+\sym{beta}_{12}\sym{x}_1\sym{x}_2+\sym{beta}_{11}\sym{x}^2_1+\sym{beta}_{22}\sym{x}^2_2+\sym{epsilon}.
\end{equation}
Die Gleichung der Modellierung kann so zur einfacheren Handhabung auch in Matrixnotation notiert werden und resultiert in:
\begin{equation} \label{eq:matrixmod}
    \sym{Y}=\sym{X}\sym{Beta} + \sym{Epsilon},
\end{equation}
wobei $\sym{Beta}$ als $\sym{p}\times 1$ Vektor der Regressionskoeffizienten ($\sym{p}=\sym{k}+1$) durch den $\sym{n}\times 1$ Vektor $\sym{Y}$ aller Beobachtungen sowie durch die Versuchsplan-Matrix $\sym{X}$ ($\sym{n}\times\sym{p}$ Einträge) unter Zuhilfenahme eines geeigneten Schätzverfahrens (vgl. Abschnitt~\ref{subsec:schätzer}) zu ermittelten ist \cite{Yang.2007,Nelson.2005}.
Sollen zunächst perspektivisch relevante Faktoren für eine versuchstechnische Untersuchung identifiziert werden, kann ein \textbf{Parameter-Screening} durchgeführt werden.
Dessen Durchführung kann sowohl heuristisch als auch versuchstechnisch erfolgen.

\subsubsection{Parameter-Screening} \label{subsubsec:screening}
Angesichts der potenziell hohen Komplexität durch Wechselwirkungen und Nichtlinearitäten sind die in Abbildung~\ref{fig:ma_abb2.02_doe_steps} beschriebenen \textit{Schritte~2-3} als propädeutische Arbeiten für ein effizientes Testdesign zu interpretieren.
Methodisch lassen sich diese unter dem Terminus \textbf{Screening} subsumieren.
Screening-Schritte sind zwischen der Definition des Untersuchungsziels und der Durchführung der physischen Screening-Experimente angeordnet (vgl. \textit{Schritt~3} in Abbildung~\ref{fig:ma_abb2.02_doe_steps} sowie Abschnitt~\ref{subsec:pläne}).
Daraus folgend dienen Screening-Methoden und -Versuchspläne dem Ziel, Informationsverluste bei einer minimalen Anzahl an Versuchsläufen zu begrenzen und die vitalen (\textit{Steuergrößen}) von den trivialen (\textit{Störgrößen}) Faktoren zu separieren, vgl. Abbildung~\ref{fig:ma_abb2.03_p_diagramm}.

Im Hinblick auf die Realisierung eines unter Zeit- und Kostenrestriktionen hochgradig effizienten \ac{DoE} ist die effiziente Ausgestaltung der Screening-Strategie selbst schon von primärem Interesse.
In traditionellen \ac{DoE}-Ansätzen impliziert dies den Einsatz von \textbf{Kreativmethoden}, wie sie Standardliteratur von \textcite{Montgomery.2020} aufführen oder exemplarisch durch \textcite{Kremer.2021} und \textcite{Gundlach.2004} zusammengefasst werden.
Hierbei ist ein Rückgriff auf Ergebnisse aus Experimenten, die explizit für das Forschungsziel ausgelegt wären, in dieser Phase unter Umständen noch nicht möglich.
Es gilt damit zunächst, qualitativ eine rein rational erlesene Sammlung an potenziellen Einflussparameter zu erstellen, um diese dann anhand ihrer geschätzten Einflüsse auf die Systemantwort zu priorisieren.
Ansätze aus der Kreativmethodik können dazu genutzt werden und fundieren auf der technischen \textbf{Systemanalyse}, die sowohl mit als auch ohne spezifisches Vorwissen über das System erfolgen kann \cite{Bertsche.2022}.
Hilfsmittel zur Priorisierung einer hier erstellten Parametersammlung können beispielsweise Entscheidungsfindungsprotokolle, wie die \ac{DSM}, und Methoden aus dem Komplexitätsmanagement, z.B. \textbf{Ishikawa-Diagramm}, sein - siehe hierzu auch weiterführende Werke von \textcite{Mayers.1997}, \textcite{Pahl.2007},\textcite{Wu.2021, Daenzer.2002} sowie \textcite{Lindemann.2008}.
Das Screening liefert somit eine rational festgestellte Auswahl an möglichst wenigen Einflussparametern, die mutmaßlich den entscheidenden Anteil an statistisch begründeter Manipulation der Systemantwort tragen und sich daher für eine Untersuchung in Versuchsplänen qualifizieren.
Entsprechend ist daraufhin ein geeigneter Versuchsplan für die physischen Datenerhebungen zu wählen.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Statistische Versuchsplanung zur Lebensdauererprobung} \label{subsec:pläne}
Standardprotokolle aus dem \ac{DoE} wie der \textbf{$2^{\sym{k}}$ voll-faktorielle Versuchsplan} eignen sich grundsätzlich auch für Lebensdaueruntersuchungen, da sich hier analog zu vergleichbar statistisch verteilter Datenlage Effekte stets als (Mittelwert-) Unterschiede in der Beobachtung der Systemantwort aus dem Vergleich zweier Einstellstufen eines oder mehrerer Faktoren ergeben \cite{Kleppmann.2020}.

\subsubsection{$2^{\sym{k}}$ Faktorielle Versuchspläne} \label{subsubsec:voll}
Demzufolge kann auch ein Lebensdauer-beeinflussendes Parameterset -  beispielhaft $(\sym{x}_1,\sym{x}_2)$ - voll-faktoriell auf zwei Stufen variiert und vollständig kombiniert werden, vgl. Abbildung~\ref{fig:abb2.05_vfdesign}.
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \setlength\figurewidth{0.9\linewidth}
        \setlength\figureheight{\figurewidth}
        \input{plots/ma_abb2.05_FF_design.tex}
        \caption{Versuchsplan}
        \label{fig:abb2.05.1_vfdesign_plot}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.55\textwidth}
        \centering
        \vspace*{1cm}
        \begin{tabular}{cccc}
            \toprule
            \multicolumn{1}{c}{Faktorstufen-} & \multicolumn{3}{c}{Faktoren und}                                         \\
            \multicolumn{1}{c}{Kombination}   & \multicolumn{3}{c}{Wechselwirkung}                                       \\
            \cmidrule(r){1-1} \cmidrule(l){2-4}
            \#                                & $\sym{x}_1$                        & $\sym{x}_2$ & $\sym{x}_1 \sym{x}_2$ \\
            \midrule
            1                                 & $-1$                               & $-1$        & $+1$                  \\
            2                                 & $+1$                               & $-1$        & $-1$                  \\
            3                                 & $-1$                               & $+1$        & $-1$                  \\
            4                                 & $+1$                               & $+1$        & $+1$                  \\
            \bottomrule
        \end{tabular}
        \vspace*{1.5cm}
        \caption{Versuchsplanmatrix zu Abb.~\ref{fig:abb2.05.1_vfdesign_plot}}
        \label{fig:abb2.05.2_vfdesign_matrix}
    \end{subfigure}
    \caption{Standard voll-faktorieller Versuchsplan}
    \label{fig:abb2.05_vfdesign}
\end{figure}
Ein derartiges Setup erlaubt es, die perspektivische Differenz erreichbarer \ac{EoL}-Werte durch tiefe und hohe Beanspruchungswerte der Faktoren zu beobachten \cite{Yang.2007,Meeker.2022}.
Der voll-faktorielle Versuchsplan bildet somit den Standard-Versuchsplan im \ac{DoE} und fordert bei einmaliger Durchführung (\textbf{Replikation} $\sym{m}=1$)
\begin{equation}
    \sym{n}=2^{\sym{k}}
    \label{eq:ffvp_n}
\end{equation}
Versuche.
Dieser Stichprobenumfang stellt sicher, dass das resultierende Gleichungssystem \textbf{gesättigt} ist: Mit $\sym{n}$ Versuchen lassen sich $\sym{n}-1$ Effekte für Hauptfaktoren und Wechselwirkungen eindeutig bestimmen.
Von entscheidender Bedeutung für die Aussagekraft des Versuchsplans ist die Wahl der Faktorstufen (vgl. Abbildung~\ref{fig:ma_abb2.06_effektabstand}).
Die Differenz der gewählten Level muss bereits im Vorfeld definiert werden, sodass signifikante Effekte sicher detektiert werden („\textbf{Signal-to-Noise}“), wobei gleichzeitig zu geringe Abstände (Rauschen) sowie zu große Intervalle (Gefahr unerkannter Nichtlinearitäten) zu vermeiden sind - vgl. Abbildung~\ref{fig:ma_abb2.06_effektabstand} sowie \textcite{Wu.2021,Siebertz.2017,Kleppmann.2020}.
\begin{figure}[htbp]
    \centering
    \setlength\figurewidth{0.9\textwidth}
    \setlength\figureheight{4cm}
    \input{plots/ma_abb2.06_effektabstand.tex}
    \caption{Einfluss der Schrittweite auf die Approximation des Effekts $\sym{Eff}$}
    \label{fig:ma_abb2.06_effektabstand}
\end{figure}
Auf Basis eines solchen zweistufigen Setups lässt sich der Zusammenhang zwischen Einflussgrößen und Lebensdauer interpolieren und in einem linearen Modell abbilden, welches zudem die Schätzung von Wechselwirkungen erlaubt.
Werden davon abweichende Modellterme zur Abbildung der Systemantwort erwartet, berücksichtigen alternative Versuchspläne typischerweise drei bis fünf Faktorstufen.
Der voll-faktorielle Versuchsplan nimmt dabei eine entscheidende Schlüsselrolle in der strategischen Modellbildung ein - insbesondere im Hinblick auf Lebensdauerdaten und Zuverlässigkeitstechnik.\
Darin aufgeführte Versuchspunkte können auf Basis der Struktur ihrer Zuordnung üblicherweise ideal aus Voruntersuchungen übernommen oder durch weiterführende Untersuchungen nachfolgend erweitert werden.
Ausgehend vom qualitativen Parameter-Screening (vgl. Abschnitt~\ref{subsec:begriffedoe}) ist ohne initiale Experimente oft unklar, welche Faktoren die Antwortvariable, also beispielsweise die Lebensdauer eines Systems, nun tatsächlich signifikant beeinflussen.
Folglich ist es essenziell, diese Fragestellung vor der eigentlichen Versuchsplanumsetzung effizient zu klären.

\subsubsection{$2^{\sym{k}-\sym{p_f}}$ Fraktionell Faktorielle Versuchspläne}
Sind nach Anwendung der Kreativmethoden (qualitatives Screening) weiterhin so viele Einflussfaktoren als relevant eingestuft, dass ein voll-faktorieller Ansatz gemäß Gleichung~\ref{eq:ffvp_n} zu einem wirtschaftlich nicht vertretbaren Versuchsumfang führen würde, muss die Strategie hin zu physikalischen Screening-Tests verschärft werden.
Dies empfiehlt sich insbesondere für Systeme mit $\sym{k} > 5$ Faktoren, um die experimentelle Effizienz zu gewährleisten.
Zur Veranschaulichung der Notwendigkeit: Bereits eine einzelne Replikation eines voll-faktoriellen Experiments mit $\sym{k}=8$ Faktoren würde $2^8 = 256$ Versuchsdurchläufe erfordern, was in der Lebensdauererprobung meist illusorisch ist.

Für derartige Selektionsaufgaben eignen sich daher \textbf{Screening-Versuchspläne}, wie der \textbf{teil-faktorielle Versuchsplan} (Fractional Factorial Design) oder alternativ der \textbf{Plackett-Burman-Plan} \cite{Kleppmann.2020,Siebertz.2017,Montgomery.2020}.
Bei diesem Ansatz wird lediglich eine selektive Teilmenge (Fraktion) der voll-faktoriellen Versuchsagenda umgesetzt, um mit minimalem Informationsverlust die für den Anwendungsfall signifikanten Effekte zu beschreiben.
Mathematisch wird die Anzahl der Versuche dabei auf
\begin{equation} \label{eq:teilfakt_n}
    \sym{n} = 2^{\sym{k}-\sym{p_f}}
\end{equation}
reduziert, wobei $\sym{p_f}$ den Grad der Fraktionierung (die Anzahl der Generatoren) angibt.
Die Validität dieses Vorgehens stützt sich auf zwei fundamentale empirische Postulate \cite{Montgomery.2021,Rigdon.2022}:
\begin{itemize}
    \item Die \textbf{Effekthierarchie} besagt, dass Effekte niedrigerer Ordnung -- primär Haupteffekte -- in der Regel eine größere Amplitude aufweisen und mit höherer Wahrscheinlichkeit signifikant sind als Effekte höherer Ordnung.
    \item Die \textbf{Effektvererbung} impliziert, dass das Auftreten signifikanter Wechselwirkungen oder quadratischer Terme strukturell an die Signifikanz ihrer korrespondierenden Haupteffekte gekoppelt ist.
\end{itemize}
Eine direkte Konsequenz dieser Reduktion („Der Preis der Einsparung“) ist jedoch, dass sich in teil-faktoriellen Versuchsplänen bestimmte Effekte nicht mehr isoliert betrachten lassen (vgl. Abbildung~\ref{fig:abb2.05.1_vfdesign_plot} für den Fall einer Fraktionierung).
So sind beispielsweise Haupteffekte unter Umständen nicht mehr zweifelsfrei von Wechselwirkungen höherer Ordnung zu unterscheiden. Da sich diese Effekte statistisch überlagern, spricht man von einer \textbf{Vermengung} (engl. \textbf{Aliasing}).
Die Schwere dieser Vermengung wird dabei über die \textbf{Auflösung} (engl. Resolution) des Versuchsplans klassifiziert (z.\,B. Auflösung III, IV oder V).
Wird dieser Informationsverlust jedoch bewusst in Kauf genommen und ingenieurwissenschaftlich bewertet, ermöglicht dies eine signifikante Reduktion des Versuchsumfangs, um effizient die dominanten Faktoren aus der initialen Parametermenge zu isolieren.
Diese Vorgehensweise ist von hoher Relevanz, da Lebensdauertests -- ob als beschleunigte Prüfung mittels \ac{ALT} oder unter Feldbedingungen -- durch die inhärente Zeitabhängigkeit der Systemantwort erhebliche Kapazitäten binden und Ergebnisse nicht ad hoc verfügbar sind.
Im Sinne einer ressourceneffizienten Gesamtstrategie sollten die Screening-Versuche daher idealerweise so konzipiert sein, dass sie nahtlos in einen nachfolgenden, höher aufgelösten Versuchsplan integriert (\textbf{augmentiert}) werden können.

\subsubsection{Wirkungsflächenversuchspläne} \label{subsubsec:rsm}
Die bisher diskutierten Versuchspläne beschränken sich auf die Untersuchung von Faktoren auf jeweils zwei Stufen ($\pm 1$). Dies ermöglicht zwar eine effiziente Darstellung linearer Beziehungen und Interaktionen, jedoch ist die Modellierung komplexerer, nicht-linearer Effekte aufgrund fehlender Stützstellen im Versuchsraum damit physikalisch nicht möglich.
Perspektivisch ist daher entscheidend, wie die bestehende Datenbasis weitergenutzt und augmentiert werden kann, falls die Systemantwort signifikante \textbf{Krümmungen} (engl. \textbf{Curvature}) aufweist und die quantitativen Beziehungen zwischen Faktoren und Zielgröße für eine Optimierung detaillierter beschrieben werden müssen.

Die \textbf{\ac{RSM}} behandelt als Teildisziplin des \ac{DoE} derartige Herangehensweisen und bietet hierfür spezielle \textbf{Wirkungsflächenversuchspläne} - engl. \textbf{\acp{RSD}} - an.
Der erste Schritt zur Detektion von Nichtlinearitäten besteht in der Integration von $\symsub{n}{idx_C}$ sogenannten \textbf{Zentralpunkten}, engl. \textbf{\acp{CP}}, in den faktoriellen Basisplan. Hierbei werden alle Faktoren auf die kodierte Stufe \textit{0} (die Mitte des Versuchsraums) gesetzt.
Weicht der Mittelwert der Systemantwort in den Zentralpunkten signifikant vom Mittelwert über die faktoriellen Eckpunkte ab, deutet dies auf eine Krümmung der Antwortfläche hin \cite{Montgomery.2020}.

Um diese quadratischen Zusammenhänge explizit zu bestimmen, gilt der \textbf{Zentral-Zusammengesetzte-Versuchsplan}, engl. \textbf{\ac{CCD}}, als etablierter Standard.
Ein \ac{CCD} entsteht durch die Augmentierung des ursprünglichen voll- (oder teil-)faktoriellen Plans (den $\symsub{n}{idx_F}$ Eckpunkten), vgl. Abbildung~\ref{fig:abb2.07_ccd_design}, um:
\begin{itemize}
    \item eine definierte Anzahl $\symsub{n}{idx_C}$ an wiederholten Zentralpunkten (üblicherweise $3 \leq \symsub{n}{idx_C} \leq 5$ zur Abschätzung des reinen Fehlers) sowie
    \item $\symsub{n}{idx_S} = 2 \cdot \sym{k}$ zusätzliche \textbf{Sternpunkte} (engl. \textbf{Axial Points} oder \textbf{Star Points}), die auf den Achsen des Koordinatensystems im Abstand $\pm \symsub{alpha}{idx_D}$ vom Zentrum liegen.
\end{itemize}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \vspace*{0.5cm}
        \setlength\figurewidth{0.9\linewidth}
        \setlength\figureheight{\figurewidth}

        \input{plots/ma_abb2.07_CCD_design.tex}
        \caption{Versuchsplan (CCD)}
        \label{fig:abb2.07.1_ccd_plot}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.55\textwidth}
        \centering
        \vspace*{0.1cm}
        \begin{tabular}{ccc}
            \toprule
            \multicolumn{1}{c}{Versuchs-} & \multicolumn{2}{c}{Faktorstufen}                            \\

            \#                            & $\sym{x}_1$                      & $\sym{x}_2$              \\
            \midrule

            1                             & $-1$                             & $-1$                     \\
            2                             & $+1$                             & $-1$                     \\
            3                             & $-1$                             & $+1$                     \\
            4                             & $+1$                             & $+1$                     \\
            5                             & $-\symsub{alpha}{idx_D}$         & $0$                      \\
            6                             & $+\symsub{alpha}{idx_D}$         & $0$                      \\
            7                             & $0$                              & $-\symsub{alpha}{idx_D}$ \\
            8                             & $0$                              & $+\symsub{alpha}{idx_D}$ \\
            9                             & $0$                              & $0$                      \\
            10                            & $0$                              & $0$                      \\
            11                            & $0$                              & $0$                      \\
            12                            & $0$                              & $0$                      \\
            13                            & $0$                              & $0$                      \\
            \bottomrule
        \end{tabular}
        \vspace*{0.5cm}
        \caption{Versuchsplanmatrix (Stufen) zu Abb.~\ref{fig:abb2.07.1_ccd_plot}}
        \label{fig:abb2.07.2_ccd_matrix}
    \end{subfigure}
    \caption{Zentral zusammengesetzter Versuchsplan (CCD) mit $\sym{k}=2$}
    \label{fig:abb2.07_ccd_design}
\end{figure}
Während die Zentralpunkte das Vorhandensein quadratischer Effekte validieren, ermöglichen die Sternpunkte deren Wertbestimmung.
Die Wahl des Abstands $\symsub{alpha}{idx_D}$ wird primär durch die Geometrie des interessierenden Versuchsraums (\textbf{Region of Interest}) diktiert.
Betrachtet man diesen Raum als Kugel (\textbf{sphärisch}), ist die \textbf{Drehbarkeit} (engl. \textbf{Rotatability}) ein wesentliches Qualitätsmerkmal.
Sie stellt sicher, dass die Varianz der Vorhersage nur vom Abstand zum Zentrum abhängt und invariant gegenüber einer Rotation des Koordinatensystems ist.
Um diese Eigenschaft zu gewährleisten, berechnet sich der Abstand $\symsub{alpha}{idx_D}$ in Abhängigkeit der faktoriellen Versuchspunkte $\symsub{n}{idx_F}$ zu \cite{Box.1957,Montgomery.2020}:
\begin{equation} \label{eq:ccd_alpha}
    \symsub{alpha}{idx_D} = (\symsub{n}{idx_F})^{1/4}.
\end{equation}
Alternativ kann für sphärische Räume auch $\symsub{alpha}{idx_D} = \sqrt{\sym{k}}$ gewählt werden (\textbf{Sphärisches \ac{CCD}}), wodurch alle Versuchs- und Sternpunkte auf einer Kugeloberfläche liegen \cite{Myers.2016}.
Ist der Versuchsraum hingegen durch harte physikalische Grenzen (z.B. maximale Temperatur) kubisch beschränkt, bietet sich der \textbf{flächenzentrierte \ac{CCD}} (\textbf{Face Centered \ac{CCD}}) an.
Hierbei wird $\symsub{alpha}{idx_D} = 1$ gesetzt, sodass die Sternpunkte direkt auf den Flächenmitten des Würfels liegen.
Dies vereinfacht die Durchführung, da nur drei Faktorstufen ($-1$, $0$, $+1$) benötigt werden, opfert jedoch die Eigenschaft der Rotierbarkeit.
Eine effiziente Alternative zum \ac{CCD} stellt das \textbf{Box-Behnken-Design} dar \cite{Box.1960}.
Dieses Design kombiniert $2^{\sym{k}}$-Faktorielle mit unvollständigen Blockplänen und platziert Versuchspunkte auf den Kantenmitten des Versuchsraums, vermeidet jedoch die extremen faktoriellen Versuchspunkte.
Dies kann vorteilhaft für Lebensdauertests sein, bei denen extreme Ecken oft zu verfrühten Ausfällen führen können.
Zudem ist es bei korrekter Wahl der Zentralpunkte alias-optimal gegenüber kubischen Modellen \cite{Jones.2011}.
Stoßen Standard-Designs (\ac{CCD}, Box-Behnken) aufgrund von Restriktionen im Versuchsraum (\textbf{Constraints}), nicht-standardmäßigen Modellierungszielen oder ungewöhnlichen Stichprobenumfängen an ihre Grenzen, empfiehlt sich der Einsatz von \textbf{Optimalen Versuchsplänen}, vergleiche übernächsten Abschnitt.
Diese computergenerierten Designs minimieren algorithmisch die durchschnittliche Vorhersagevarianz über den gesamten Designraum und können Standard-Designs in ihrer Prädiktionsgüte oft übertreffen \cite{Goos.2011,Montgomery.2020}.

\subsubsection{Strategische Vorgehensweisen}
In der Praxis entsteht ein \ac{CCD} häufig im Rahmen einer \textbf{sequenziellen Versuchsstrategie}: Zeigt das initiale lineare Modell Anpassungsmängel (\textbf{Lack-of-Fit}), wird der bestehende faktorielle Plan um die Sternpunkte augmentiert, um ein Modell zweiter Ordnung zu fitten \cite{Myers.2016}.
Modelle zweiter Ordnung sind insbesondere in der Lebensdauer- und Zuverlässigkeitstechnik vermehrt von Bedeutung, da sie die Existenz von lokalen Extrema (Minima oder Maxima) der Systemantwort im Versuchsraum detaillierter abbilden können \cite{Rigdon.2022,Dean.2017}.
Prinzipielle Vorgehensweisen, die ein erfolgreiches Umsetzen von \ac{DoE} bzw. \ac{L-DoE} begünstigen sollten, wie \textbf{Blockbildung} oder \textbf{Randomisierung} (vgl. \cite{Siebertz.2017,Kleppmann.2020}) seien hier bereits vorausgesetzt.
Abbildung~\ref{fig:ma_abb2.08_strategy} fasst eine derartige strategische Vorgehensweise zur Realisierung eines \ac{CCD} zusammen, wie sie beispielsweise \textcite{Box.1988,Bisgaard.1992} vorschlagen.

\begin{figure}[htbp]
    \centering

    \setlength\figurewidth{0.15\textwidth}
    \setlength\figureheight{0.15\textwidth}

    % --- 1. Teilfaktoriell ---
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \input{plots/ma_abb2.08.1_strategy.tex}
    \end{minipage}
    \hfill
    % --- Pfeil 1 ---
    \begin{minipage}[b]{0.05\textwidth}
        \centering
        \raisebox{2.25cm}{
            \hspace*{0.3cm}
            \tikz \draw[-latex, line width=1.5pt] (0,0) -- (0.4,0);
        }
    \end{minipage}
    \hfill
    % --- 2. Vollfaktoriell ---
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \input{plots/ma_abb2.08.2_strategy.tex}
    \end{minipage}
    \hfill
    % --- Pfeil 2 ---
    \begin{minipage}[b]{0.02\textwidth}
        \centering
        \raisebox{2.25cm}{
            \hspace*{0.0cm}
            \tikz \draw[-latex, line width=1.5pt] (0,0) -- (0.4,0);
        }
    \end{minipage}
    \hfill
    % --- 3. Zentralpunkt ---
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \input{plots/ma_abb2.08.3_strategy.tex}
    \end{minipage}
    \hfill
    % --- Pfeil 3 ---
    \begin{minipage}[b]{0.01\textwidth}
        \centering
        \raisebox{2.25cm}{
            \hspace*{0.00cm}
            \tikz \draw[-latex, line width=1.5pt] (0,0) -- (0.4,0);
        }
    \end{minipage}
    \hfill
    % --- 4. CCD ---
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \input{plots/ma_abb2.08.4_strategy.tex}
    \end{minipage}
    \caption{Strategischer Ansatz zu augmentierter Versuchsplanung für die Lebensdauererprobung nach \cite{Box.1988,Bisgaard.1992}}
    \label{fig:ma_abb2.08_strategy}
\end{figure}
Um effizient Versuchspunkte zu allokieren und für sequentiell fortlaufende Versuche weiterverwendet zu werden, wird in Abgleich mit Abbildung~\ref{fig:ma_abb2.02_doe_steps} zunächst ein \textbf{teil-faktorieller Versuchsplan} zur Identifikation signifikanter Faktoren durchgeführt.
Anschließend wird der Plan zu einem \textbf{voll-faktoriellen Versuchsplan} erweitert, um alle Haupteffekte und Wechselwirkungen erster Ordnung zu schätzen.
Daraufhin werden \textbf{Zentralpunkte} ergänzt, um das Vorhandensein von Krümmungen zu validieren.
Abschließend werden bei Bedarf die \textbf{Sternpunkte} hinzugefügt, um ein vollständiges \ac{CCD} zu realisieren.
Insbesondere in der Lebensdauererprobung ist eine derartige schrittweise Vorgehensweise sinnvoll, um den Versuchsumfang zu minimieren und dennoch eine fundierte Modellierung der Systemantwort zu gewährleisten.
Gleichzeitig erlaubt diese Strategie eine flexible Anpassung an unerwartete Ergebnisse in den einzelnen Phasen der Untersuchung, während lokale Optima von Zuverlässigkeiten im Sinne von \ac{DfR} im Vorfeld ohnehin nicht bekannt sind.
So kann beispielsweise auf Basis der Ergebnisse des teil-faktoriellen Plans entschieden werden, ob eine Erweiterung zum voll-faktoriellen Plan überhaupt notwendig ist, oder ob direkt zu den Zentralpunkten übergegangen wird.
Genauso kann die Anzahl der Zentralpunkte an die beobachtete Streuung der Lebensdauerdaten angepasst werden, um eine robuste Schätzung des Fehlers zu gewährleisten.
Ist dann eine Extrapolation in Richtung a-priori unbekannter Beanspruchungsniveaus geplant, verfügt der \ac{CCD} über die notwendigen Stützstellen, um eine adäquate Exploration des Versuchsraums zu ermöglichen.

\subsubsection{Metriken zur optimalen Versuchsplanung}
Wird maximale Flexibilität jenseits starrer Standard-Designs wie dem voll-faktoriellen Setup oder dem \ac{CCD} verlangt, können \textbf{optimale Versuchspläne} einen leistungsfähigen, \textit{effizienten} Lösungsansatz für \textit{multivariate} Versuchsplanung bieten \cite{Kleppmann.2016,Montgomery.2020,Myers.2016}.
Diese verfolgen das Ziel, die Versuchspunkte algorithmisch so im Versuchsraum zu positionieren, dass spezifische Qualitätskriterien - allen voran die Verteilung der (um die Versuchsanzahl $\sym{N}$ \textbf{skalierten}) \textbf{Prädiktionsvarianz}, engl. \textbf{\ac{SPV}},
\begin{equation}
    \sym{pred_var}= \frac{\sym{N} \sym{Var}[\hat{\sym{y}}(\symsub{x_loc}{idx_0})]}{\sym{sigma_sq}}
    = \symsub{x_loc}{idx_0}' \left( \frac{\sym{M}}{\sym{N}} \right)^{-1} \symsub{x_loc}{idx_0}
    = \sym{N} \symsub{x_loc}{idx_0}' \sym{M}^{-1} \symsub{x_loc}{idx_0}
    \label{eq:pred_variance_scaled}
\end{equation}
optimiert werden \cite{Myers.2016,Montgomery.2020}.
Die Prädiktionsvarianz ist ein dimensionsloses Maß, das ausschließlich über die geometrische Anordnung der \sym{N} Versuchspunkte (der Matrix $\sym{X}$) in der \textbf{Informationsmatrix} (auch \textbf{\textit{M}omentenmatrix})
\begin{equation}
    \sym{M}\stackrel{\sym{m}=1}{=} \sym{X}'\sym{X}
    \label{eq:informationsmatrix}
\end{equation}
definiert und damit positionsabhängig von einer Referenz $\symsub{x_loc}{idx_0}'=[1,\sym{x}_{01},\cdots,\sym{x}_{0\sym{k}}]$ im Parameterraum ist \cite{Myers.2016}.
Sie ist damit unabhängig von der tatsächlichen Streuung der Messdaten ($\sym{sigma_sq}$).
Weiter wird sie über die Anzahl der Versuchspunkte $\sym{N}$ skaliert, um Vergleiche zwischen Versuchsplänen mit unterschiedlicher Stichprobengröße zu ermöglichen \cite{Goos.2011,Myers.2016} - kann aber auch unskaliert betrachtet werden: als \textbf{\ac{UPV}} $=\sym{Var} [\hat{\sym{y}}(\symsub{x_loc}{idx_0})] / \sym{sigma_sq} = \symsub{x_loc}{idx_0}' \sym{M}^{-1} \symsub{x_loc}{idx_0}$, vgl. \textcite{Montgomery.2020,Myers.2010}.
Zusammen mit der Varianz des Versuchsfehlers $\sym{sigma_sq}$ (vgl. Gleichung~\ref{eq:dispersion}) erlaubt die Prädiktionsvarianz somit eine direkte Quantifizierung der \textbf{Unsicherheit in der Modellvorhersage} $\sym{Var} [\hat{\sym{y}}(\symsub{x_loc}{idx_0})]$ an einer beliebigen Stelle $\symsub{x_loc}{idx_0}$ im Versuchsraum.
Eine effiziente Versuchsplanung für multivariate Lebensdaueruntersuchungen muss sich neben pragmatischen Beweggründen maßgeblich an diesen Größen orientieren, weshalb im Folgenden eine Übersicht der gängigen Metriken und Qualitätsmerkmale gegeben wird.\


Der Einsatz optimaler Versuchspläne ist zunächst prädestiniert für Szenarien, in denen klassische Pläne an ihre Grenzen stoßen - sei es durch physikalische Randbedingungen welche bestimmte Faktorstufenkombinationen ausschließen, oder durch strikte Limitierungen der verfügbaren Versuchskapazität \cite{Goos.2011,Kleppmann.2020}.
Derart klassische Versuchspläne wie der voll-faktorielle Versuchsplan sind aufgrund ihrer Versuchspunktanordnung stets \textbf{orthogonal} \cite{Rigdon.2022,Montgomery.2020}.
Ein Versuchsplan wird als orthogonal bezeichnet, wenn keine Korrelation zwischen jeweils zwei Spalten der Versuchsplan-Matrix vorliegt - deren Skalarprodukte also jeweils null ergeben:
\begin{equation}
    \langle \symsub{x}{idx_i}, \symsub{x}{idx_j} \rangle = 0 \quad \text{für alle } \sym{idx_i} \neq \sym{idx_j}.
    \label{eq:skalar}
\end{equation}
Oder in anderen Worten: entspricht die Informationsmatrix $\sym{M}$ einer Diagonalmatrix, ist der Versuchsplan orthogonal \cite{Montgomery.2021,Rigdon.2022}.
So können Effekte eindeutig identifiziert werden - die Vektoren der Faktorstufenkombinationen sind linear unabhängig und Effekte lassen sich unverzerrt schätzen.
Dies stellt einen Versuchsplan also zunächst einmal qualitativ günstig dar.


Zudem liegt \textbf{Ausgewogenheit} vor, sofern für einen jeweiligen Faktor alle anderen Faktoreinstellungen gleichmäßig aufgeteilt sind \cite{Siebertz.2017}.
Damit wird Varianzhomogenität und Gleichbehandlung der Faktoren gewährleistet, sodass die Schätzung der Effekte unverzerrt erfolgt.

Mathematisch fundiert die Bewertung der Schätzgenauigkeit auf der Inversen der Informationsmatrix, welche als \textbf{Varianz-Kovarianz-Matrix} (oder \textbf{Dispersionsmatrix}) $(\sym{X}'\sym{X})^{-1}$ der Regressionskoeffizienten definiert ist:
\begin{equation}
    \sym{Var}(\hat{\sym{Beta}}) = \sym{sigma_sq} (\sym{X}'\sym{X})^{-1},
    \label{eq:dispersion}
\end{equation}
wobei wie gehabt $\sym{sigma_sq}$ die Varianz des Versuchsfehlers darstellt.
Die Diagonalelemente der Matrix $(\sym{X}'\sym{X})^{-1}$ stehen hierbei in direktem Zusammenhang mit dem \textbf{\ac{VIF}} \cite{Siebertz.2017,Kleppmann.2020}.
Der \ac{VIF} dient als Maßzahl für Multikollinearität und quantifiziert den Faktor, um den sich die Varianz eines geschätzten Koeffizienten im Vergleich zu einem vollständig orthogonalen Design aufgrund von Korrelationen zwischen den Faktoren erhöht (vergleiche auch \textbf{Konditionszahl}) \cite{Hedderich.2020}.
Während bei orthogonalen Plänen (Idealfall) ein $\text{\ac{VIF}} = 1$ vorliegt, deuten hohe Werte (typischerweise $>5$ oder $>10$) auf eine instabile Modellschätzung hin \cite{Kleppmann.2020,Montgomery.2021}.


Ergänzend zur globalen Bewertung der Multikollinearität durch den \ac{VIF} erlaubt die Betrachtung der sogenannten \textbf{Prädiktionsmatrix} (engl. auch \textbf{Hat-Matrix})
\begin{equation}
    \sym{Hat} = \sym{X}(\sym{X}'\sym{X})^{-1}\sym{X}'
    \label{eq:hat_matrix}
\end{equation}
eine lokale Diagnose der Versuchspunkte.
Die Diagonalelemente $\sym{h}_{\sym{idx_i}\sym{idx_i}} \in \left[ \frac{1}{\sym{n}}, 1 \right]$ dieser Projektionsmatrix, bezeichnet als \textbf{Hebelwerte} (engl. \textbf{Leverage}), quantifizieren den Einfluss eines einzelnen Versuchslaufs auf die Modellvorhersage.
Punkte mit hohen Hebelwerten (typischerweise $\sym{h}_{\sym{idx_i}\sym{idx_i}}> 2\sym{p}/\sym{n}$) befinden sich geometrisch weit vom Zentrum des Versuchsraums entfernt und dominieren die Regression, was den Plan anfällig für Ausreißer in diesen spezifischen Einstellungen macht \cite{Siebertz.2017,Montgomery.2021}.

Während Hebelwerte jedoch lediglich das \textit{Potenzial} einer Beobachtung zur Modellbeeinflussung aufgrund ihrer geometrischen Exponiertheit indizieren, quantifizieren Einflussstatistiken die \textit{tatsächliche} Auswirkung auf die Regressionsparameter und die Vorhersagegüte.
Die Identifikation solcher Beobachtungen erfolgt methodisch durch den iterativen Ausschluss des $i$-ten Datensatzes (\textit{Leave-One-Out}-Methodik) und den Vergleich der resultierenden Modellstatistiken mit dem ursprünglichen Modell basierend auf $\sym{n}$ Beobachtungen \cite{Belsley.2004, Montgomery.2021}.
Als globales Maß für den Einfluss der $i$-ten Beobachtung auf den Vektor aller geschätzten Regressionskoeffizienten $\hat{\sym{Beta}}$ dient die \textbf{Cook-Distanz} ($\sym{D}_i$).
Sie verknüpft die Information des intern studentisierten Residuums $\symsub{resid}{idx_i}$ mit dem Hebelwert $\sym{h}_{\sym{idx_i}\sym{idx_i}}$:
% ...
\begin{equation}
    \symsub{D}{idx_i} = \frac{\sym{resid}_{\sym{idx_i}}^2}{\sym{p}} \cdot \frac{\sym{h}_{\sym{idx_i}\sym{idx_i}}}{1-\sym{h}_{\sym{idx_i}\sym{idx_i}}}.
    \label{eq:cooks_d}
\end{equation}
% ...
Ein hoher Wert (typischerweise $\symsub{D}{idx_i} > 1$ oder $\symsub{D}{idx_i} > 4/\sym{n}$) signalisiert, dass das Entfernen der Beobachtung zu einer signifikanten Verschiebung der Modellparameter führen würde, da der Punkt sowohl weit vom Zentrum liegt als auch einen großen Fehler aufweist \cite{Fahrmeir.2009,Belsley.2004}.

Für eine differenzierte Analyse auf Ebene der einzelnen Modellterme wird die Metrik $\sym{dfbetas}$ (Difference in Betas, Standardized) herangezogen.
Sie misst die standardisierte Änderung eines spezifischen Regressionskoeffizienten $\sym{beta}_{\sym{idx_j}}$ bei Ausschluss der $\sym{idx_i}$-ten Beobachtung:
\begin{equation}
    \sym{dfbetas}_{\sym{idx_j},\sym{idx_i}} = \frac{\hat{\sym{beta}}_{\sym{idx_j}} - \hat{\sym{beta}}_{\sym{idx_j}(-\sym{idx_i})}}{\sqrt{\hat{\sym{sigma}}_{(-\sym{idx_i})}^2 (\sym{X}'\sym{X})^{-1}_{\sym{idx_j}\sym{idx_j}}}},
    \label{eq:dfbetas}
\end{equation}
wobei $\hat{\sym{beta}}_{\sym{idx_j}(-\sym{idx_i})}$ den Koeffizienten ohne die $\sym{idx_i}$-te Beobachtung und $\hat{\sym{sigma}}_{(-\sym{idx_i})}^2$ die entsprechende Fehlerquadratschätzung darstellt.
Hiermit lässt sich prüfen, ob einzelne Versuche die Schätzung eines Effekt-Terms verzerren (kritischer Schwellenwert oft $> 2/\sqrt{\sym{n}}$) \cite{Belsley.2004,Montgomery.2021}.

Ergänzend beschreibt $\sym{dffits}$ (Difference in Fits) die Änderung des Vorhersagewertes an der Stelle $\sym{idx_i}$ selbst, normiert auf die Standardabweichung der Anpassung:
\begin{equation}
    \sym{dffits}_{\sym{idx_i}} = \frac{\hat{\sym{y}}_{\sym{idx_i}} - \hat{\sym{y}}_{\sym{idx_i}(-\sym{idx_i})}}{\sqrt{\hat{\sym{sigma}}_{(-\sym{idx_i})}^2 \sym{h}_{\sym{idx_i}\sym{idx_i}}}}.
    \label{eq:dffits}
\end{equation}

Abschließend bewertet die $\sym{covratio}$ den Einfluss auf die Präzision der Schätzung.
Sie setzt die Determinante der Varianz-Kovarianz-Matrix ohne die $\sym{idx_i}$-te Beobachtung ins Verhältnis zur ursprünglichen Matrix:
\begin{equation}
    \sym{covratio}_{\sym{idx_i}} = \frac{\det((\sym{X}_{(-\sym{idx_i})}'\sym{X}_{(-\sym{idx_i})})^{-1})}{\det((\sym{X}'\sym{X})^{-1})}.
    \label{eq:covratio}
\end{equation}
Werte, die signifikant von $1$ abweichen (Grenzen $1 \pm 3\sym{p}/\sym{n}$), zeigen an, dass die Beobachtung die Konfidenzbereiche der Koeffizienten unverhältnismäßig beeinflusst \cite{Belsley.2004}.

Da in restriktiven Versuchsräumen Orthogonalität oft nicht erreichbar ist, zielen optimale Versuchspläne darauf ab, durch Minimierung spezifischer Eigenschaften der Dispersionsmatrix $(\sym{X}'\sym{X})^{-1}$ den Informationsgehalt trotz Korrelationen zu maximieren.
Die Generierung solcher maßgeschneiderten Pläne erfolgt algorithmisch unter Maximierung spezifischer statistischer Gütekriterien.
Eine Übersicht der einschlägigen \textbf{Optimalitätskriterien} - oder einfach \textbf{Optimalitäten} - welche den quantitativen Vergleich zur Standard-Methodik ermöglichen, ist in Tabelle~\ref{tab:2.1_optimalitaeten} zusammengefasst \cite{Goos.2011,Montgomery.2020}.
Sie bilden mitunter die Grundlage für diverse weitere Optimalitäten und hybride Ansätze und lassen sich prinzipiell in Modellierungskriterien (\textit{\textbf{A}}-, \textit{\textbf{D}}-~Optimalität) und Prädiktionskriterien (\textit{\textbf{G}}-, \textit{\textbf{I}}-, \textit{\textbf{U}}-, \textit{\textbf{V}}\textbf{-~Optimalität}) unterteilen \cite{Myers.2016,Rigdon.2022}.

Jenseits praktischer Kriterien sind hier der Vollständigkeit halber auch eine Teilmenge eher theoretischerer Optimalitäten zu nennen.
Die \textbf{\textit{E}-Optimalität}, führt zur Minimierung des maximalen Eigenwerts der Dispersionsmatrix, was eine Worst-Case-Absicherung zur Varianzschätzung begünstigt - vergleiche \textcite{Boyd.2004,Russell.2019}.
Für explorative Zielsetzungen ohne starre Modellannahmen kann nach \textcite{Atkinson.1970} die \textit{\textbf{S}}\textbf{-Optimalität} relevant sein, da hier durch die Maximierung der euklidischen Abstände benachbarter Punkte eine gleichmäßige Raumfüllung (\textit{Space-Filling}) sichergestellt werden kann.
So können für sich ändernde Modelle entlang einzelner Parameter wie beispielsweise $\hat{\sym{T}} \in \hat{\sym{theta}}$ Sensitivatsanalysen effizient durchgeführt werden.
Eine Verallgemeinerung der prädiktionsorientierten Kriterien stellt die \textit{\textbf{Q}}\textbf{-Optimalität} dar, welche analog zur \textbf{\textit{I}}-Optimalität die integrierte Prädiktionsvarianz minimiert, jedoch mittels Gewichtsfunktionen eine differenzierte Priorisierung spezifischer Regionen im Versuchsraum erlaubt, vgl. \textcite{Goos.2011}.
Liegt der Fokus hingegen isoliert auf einer Teilmenge der Modellparameter - etwa zur Trennung von Haupteffekten und Blockeffekten oder von Termen erster und zweiter Ordnung - ermöglicht die \textit{\textbf{D}}$_{\textit{\textbf{S}}}$\textbf{-Optimalität} als Derivat der \textit{\textbf{D}}-Optimalität eine gezielte Maximierung der Schätzgüte für genau dieses jeweilige \textit{S}ubset der Parametergruppe - siehe auch Werke von \textcite{Goos.2011,Atkinson.2007,Myers.2016}.

{
\renewcommand{\arraystretch}{1.2}
\newcommand{\compressEq}{\vspace*{-0.4\baselineskip}}
\begin{longtable}{@{}lp{0.75\textwidth}@{}}

    \caption{Übersicht, Zielsetzung und mathematische Definition verschiedener Optimalitätskriterien für Versuchspläne}  %
    \label{tab:2.1_optimalitaeten}                                                                              \\
    \toprule
    \textbf{Kriterium}              & \textbf{Zielgröße, Beschreibung und Definition}                           \\
    \midrule
    \endfirsthead

    \caption*{Tabelle \ref{tab:2.1_optimalitaeten} (\textit{Fortsetzung}): Übersicht der Optimalitätskriterien} \\
    \toprule
    \textbf{Kriterium}              & \textbf{Zielgröße, Beschreibung und Definition}                           \\
    \midrule
    \endhead

    \midrule
    \multicolumn{2}{r}{\footnotesize\textit{Fortsetzung auf der nächsten Seite...}}                             \\
    \endfoot

    \bottomrule
    \endlastfoot

    \textbf{\textit{A}-Optimalität} &
    Minimierung der Spur der inversen Informationsmatrix \cite{Kleppmann.2020,Montgomery.2020,Rigdon.2022}. \newline
    \textit{Ziel: Maximale Präzision der einzelnen Parameter im Durchschnitt.}
    \compressEq
    \begin{equation} \label{eq:opt_A}
        \sym{A_opt} = \min \left( \text{spur} \left(\sym{M}^{-1}\right) \right) = \min \left( \sum_{\sym{idx_j}=1}^{\sym{k}} \sym{Var}\left(\hat{\beta}_{\sym{idx_j}}\right) \right)
    \end{equation}
    \compressEq
    \\ \addlinespace

    \textbf{\textit{D}-Optimalität} &
    Maximierung der Determinante der Informationsmatrix $\sym{M}$. Da das Volumen des Vertrauensellipsoids der Koeffizientenschätzwerte umgekehrt proportional zur Quadratwurzel der Determinante ($\sqrt{\det(\sym{M})}$) ist, minimiert dieses Kriterium das Unsicherheitsvolumen im Parameterraum \cite{Montgomery.2020,Kleppmann.2020,Myers.2016}. \newline
    \textit{Ziel: Minimierung des Volumens des gemeinsamen Vertrauensbereichs aller Modellparameter.}
    \compressEq
    \begin{equation} \label{eq:opt_D}
        \sym{D_opt} = \max \left( \det(\sym{M}) \right) = \min \left( \det\left(\sym{M}^{-1}\right) \right)
    \end{equation}
    \compressEq
    \\ \addlinespace

    % \textbf{\textit{E}-Optimalität} &
    % Minimierung des maximalen Eigenwerts $\sym{eigen_val}_{\max}$ der inversen Informationsmatrix \cite{Kleppmann.2020,Boyd.2004}. \newline
    % \textit{Ziel: Worst-Case-Absicherung für den am schlechtesten geschätzten Parameter.}
    % \compressEq
    % \begin{equation} \label{eq:opt_E}
    %     \sym{E_opt} = \min \left( \sym{eigen_val}_{\max}\left(\sym{M}^{-1}\right) \right)
    % \end{equation}
    % \compressEq
    % \\ \addlinespace

    \textbf{\textit{G}-Optimalität} &
    Minimierung der maximalen skalierten Prädiktionsvarianz $\sym{pred_var}$ im relevanten Parameterraum \cite{Montgomery.2020,Myers.2016,Russell.2019}. \newline
    \textit{Ziel: Qualität der Vorhersage an der ungünstigsten Stelle sichern.}
    \compressEq
    \begin{equation} \label{eq:opt_G}
        \sym{G_opt} = \min \left( \max_{\symsub{x_loc}{idx_0} \in \sym{RR}^{\sym{k}}} \sym{pred_var} \right)
    \end{equation}
    \compressEq
    \\ \addlinespace

    \textbf{\textit{I}-Optimalität} &
    Minimierung der durchschnittlichen (\textit{integrierten}) Prädiktionsvarianz über den Parameterraum - auch als \textbf{\textit{U}-Optimalität} im ingenieurwissenschaftlich-statistischen Sinn bekannt: Optimierung bzgl. einer \textit{uniformen} Prädiktionsvarianz \cite{Monroe.2011,Rigdon.2022,Atkinson.2007}. \newline
    \textit{Ziel: Optimale Vorhersagegüte (\ac{SPV}) im Mittel über den gesamten Raum - z.B. bei initialer Parametrisierung eines multivariaten Lebensdauermodells.}
    \compressEq
    \begin{equation} \label{eq:opt_I}
        \sym{I_opt} = \min \left(\frac{\sym{N}}{\int_{\sym{RR}^{\sym{k}}} d\sym{x}} \int_{\sym{RR}^{\sym{k}}} \sym{pred_var} \, d\sym{x} \right)
    \end{equation}
    \compressEq
    \\ \addlinespace


    % \textbf{S-Optimalität}       &
    % Minimierung der Varianz der geschätzten Steigung der Regressionsfunktion an einem spezifischen Punkt $\symsub{x_loc}{idx_0}$. \newline
    % \textit{Ziel: Maximale Präzision des Grenzeffekts (Slope) an einer kritischen Stelle.}
    % \compressEq
    % \begin{equation} \label{eq:opt_S_Atkinson}
    %     \sym{S_opt} = \min_{\xi} \left( \sym{s_grad}^T \sym{M}^{-1} \sym{s_grad} \right) \text{ mit } \sym{s_grad} = \frac{\partial \sym{f}(\symsub{x_loc}{idx_0})}{\partial \symsub{x_loc}{idx_0}}
    % \end{equation}
    % \compressEq
    % \\ \addlinespace

    \textbf{\textit{V}-Optimalität} &
    Minimierung der durchschnittlichen Prädiktionsvarianz über ein diskretes Set von $\sym{N}$ Punkten \cite{Goos.2011,Montgomery.2020,Myers.2010}. \newline
    \textit{Ziel: Optimale Vorhersagegüte (\ac{SPV}) an spezifischen \sym{idx_i} Stellen - z.B. bei \ac{L-DoE}-Versuchspunkten oder spezifischen Nennlasten.}
    \compressEq
    \begin{equation} \label{eq:opt_V}
        \sym{V_opt} = \min \left( \frac{1}{\sym{N}} \sum_{\sym{idx_i}=1}^{\sym{N}} \nu(\symsub{x}{idx_i}) \right)
    \end{equation}
    \compressEq
    \\
\end{longtable}
}


Zur vergleichbaren Bewertung unterschiedlicher Designs unabhängig von der Skalierung werden normierte \textbf{Effizienzen} herangezogen \cite{Montgomery.2020}.
Aus Tabelle~\ref{tab:2.1_optimalitaeten} lassen sich die \textbf{\textit{A}-Effizienz} $\sym{eff_A}$, \textbf{\textit{D}-Effizienz} $\sym{eff_D}$, \textbf{\textit{G}-Effizienz} $\sym{eff_G}$, \textbf{\textit{I}-Effizienz} $\sym{eff_I}$ und \textbf{\textit{V}-Effizienz} $\sym{eff_V}$ ableiten, welche jeweils die Güte $[0,1]$ eines Versuchsplans quantifizieren - vergleiche Tabelle~\ref{tab:2.2_effizienzen}.

{
\renewcommand{\arraystretch}{1.2}
\newcommand{\compressEq}{\vspace*{-0.4\baselineskip}}
\begin{longtable}{@{}lp{0.75\textwidth}@{}}

    \caption{Übersicht, Zielsetzung und mathematische Definition verschiedener Effizienzkriterien für Versuchspläne}
    \label{tab:2.2_effizienzen}                                                                           \\
    \toprule
    \textbf{Kriterium}            & \textbf{Zielgröße, Beschreibung und Definition}                       \\
    \midrule
    \endfirsthead

    \caption*{Tabelle \ref{tab:2.2_effizienzen} (\textit{Fortsetzung}): Übersicht der Effizienzkriterien} \\
    \toprule
    \textbf{Kriterium}            & \textbf{Zielgröße, Beschreibung und Definition}                       \\
    \midrule
    \endhead

    \midrule
    \multicolumn{2}{r}{\footnotesize\textit{Fortsetzung auf der nächsten Seite...}}                       \\
    \endfoot

    \bottomrule
    \endlastfoot

    \textbf{\textit{A}-Effizienz} &
    Maß für die durchschnittliche Präzision der Regressionskoeffizienten. \cite{Myers.2016,Goos.2011}.  \newline
    \compressEq
    \begin{equation} \label{eq:eff_A}
        \sym{eff_A} = 100 \cdot \frac{\sym{p}}{\text{spur} (\sym{N} \cdot \sym{M}^{-1})}
    \end{equation}
    \compressEq
    \\ \addlinespace

    \textbf{\textit{D}-Effizienz} &
    Maß für den Informationsgehalt der Informationsmatrix $\sym{M}$, definiert über die Determinante, umgekehrt proportional zum Volumen des Vertrauensellipsoids der Parameterschätzwerte und durch die Potenzierung mit $1/\sym{p}$ pro Schätzparameter normiert. \cite{Myers.2016,Goos.2011}.
    \compressEq
    \begin{equation} \label{eq:eff_D}
        \sym{eff_D} = 100 \cdot \frac{\det(\sym{M})^{1/\sym{p}}}{\sym{N}}
    \end{equation}
    \compressEq
    \\ \addlinespace

    \textbf{\textit{G}-Effizienz} &
    Maß für die Vorhersagegüte im ungünstigsten Fall innerhalb des Versuchsraums. Da für die maximale \ac{SPV} die theoretische Untergrenze $\sym{pred_var}_{\max} \geq \sym{p}$ gilt, beschreibt dieses Kriterium das Verhältnis zwischen idealer und realisierter maximaler Varianz \cite{Myers.2016}.
    \compressEq
    \begin{equation} \label{eq:eff_G}
        \sym{eff_G} = 100 \cdot \frac{\sym{p}}{\sym{pred_var}_{\max}}
    \end{equation}
    \compressEq
    \\ \addlinespace

    \textbf{\textit{I}-Effizienz} &
    Maß für die durchschnittliche Vorhersagegüte über den gesamten Versuchsraum \cite{Myers.2016}.
    \compressEq
    \begin{equation} \label{eq:eff_I}
        \sym{eff_I} = 100 \cdot \frac{\sym{p}}{\displaystyle \frac{1}{\int_{R} d\sym{x}} \int_{R} \sym{pred_var} \, d\sym{x}}
    \end{equation}
    \compressEq
    \\ \addlinespace

    \textbf{\textit{V}-Effizienz} &
    Maß für die durchschnittliche Vorhersagegüte über ein diskretes Set von $\sym{N}$ Punkten analog zu Gleichung~\ref{eq:eff_I} \cite{Myers.2016}.
    \compressEq
    \begin{equation} \label{eq:eff_V}
        \sym{eff_V} = 100 \cdot \frac{1}{\sym{N}} \sum_{\sym{idx_i}=1}^{\sym{N}} \frac{\sym{p}}{\sym{pred_var}(\symsub{x}{idx_i})}
    \end{equation}
    \compressEq
    \\
\end{longtable}
}

% Die \textbf{\textit{A}-Effizienz} $\sym{eff_A}$ misst die durchschnittliche Präzision der Parameterschätzung im Vergleich zu einem idealen orthogonalen Design gleicher Größe:
% \begin{equation} \label{eq:eff_A}
%     \sym{eff_A} = 100 \cdot \frac{\sym{p}}{\text{spur} (\sym{N} \cdot \sym{M}^{-1})}.
% \end{equation}
% Die \textbf{\textit{D}-Effizienz }$\sym{eff_D}$ beschreibt die Güte der Parameterschätzung im Vergleich zu einem idealen orthogonalen Design gleicher Größe. Hohe Werte (nahe $100\,\%$) stehen für eine effiziente Ausnutzung der Versuche:
% \begin{equation} \label{eq:eff_D}
%     \sym{eff_D} = 100 \cdot \left( \frac{\det(\sym{M})^{1/\sym{p}}}{\sym{N}} \right).
% \end{equation}
% Die \textbf{\textit{G}-Effizienz} $\sym{eff_G}$ bewertet die Robustheit der Vorhersage. Sie setzt die theoretisch minimal erreichbare Prädiktionsvarianz ($\sym{p}/\sym{n}$) ins Verhältnis zum tatsächlichen Worst-Case-Wert $\sym{G_opt}$ des Designs:
% \begin{equation} \label{eq:eff_G}
%     \sym{eff_G} = 100 \cdot \frac{\sym{p}}{\sym{pred_var}_{\max}}.
% \end{equation}\


\subsubsection{Signifikanz und Trennschärfe}
Zuletzt muss sichergestellt werden, dass der Versuchsplan über eine ausreichende \textbf{Trennschärfe} (engl. $\sym{power}$) verfügt.
Die Trennschärfe beschreibt als Metrik diejenige Wahrscheinlichkeit, mit der ein existierender Einfluss durch Faktoren vor dem Hintergrund des experimentellen Rauschens (vergleiche \textbf{\sym{t}-Statistik}, \textbf{Signal-to-Noise Ratio}, \textbf{Signifikanztests} nach \textcite{Kleppmann.2020,Montgomery.2020,Goos.2011}) korrekt als \textbf{signifikanter} Effekt $\sym{Eff}$ identifiziert wird \cite{Siebertz.2017,Kleppmann.2020}.
Insofern ist ein Effekt in praktischem Umfeld natürlich problemabhängig (Streuungscharakteristik der Lebensdauer, realisierbare Stichprobengröße, etc.) und meist von Bedeutung für $\sym{Eff} \geq 2\sym{sigma}$, wobei Trennschärfewerte $\geq 80\%$ als befriedigend gelten \cite{Rigdon.2022,Montgomery.2020}.
Insbesondere bei reduzierten Versuchsplänen und unter dem Einfluss stochastischer Lebensdauerstreuung ist die Power-Analyse jedoch essenziell, um das Risiko von falsch-negativen Schlussfolgerungen zu minimieren.
Letztendlich kann die Trennschärfe somit als Garantiemetrik verstanden werden, die sicherstellt, dass ein geplanter Versuchsaufbau in der Lage ist, Effekte von praktischer Relevanz zu detektieren \cite{Cohen.1988,Myers.2016} - was vornehmlich in Bezug auf Lebensdauertests aufgrund genannter wirtschaftlicher Randbedingungen von hoher Bedeutung ist.
Formalisiert wird dies im Rahmen \textbf{statistischer Hypothesentests} \cite{Kleppmann.2020,Montgomery.2020}.
Die Zielsetzung besteht darin, den Nachweis zu erbringen, ob ein bestimmter Faktor $\symsub{x}{idx_j}$ einen signifikanten Einfluss auf die Systemantwort $\sym{y}$ ausübt.
Dazu werden zwei konkurrierende Hypothesen formuliert:
Die \textbf{Nullhypothese} $\sym{H0}$ unterstellt, dass kein Zusammenhang besteht, der korrespondierende Regressionskoeffizient $\symsub{beta}{idx_j}$ (vgl. Abschnitt~\ref{subsec:model}) also den Wert Null annimmt:
\begin{equation} \label{eq:hypotheses}
    \sym{H0}: \sym{beta}_j = 0 \quad \text{gegen} \quad \sym{H1}: \sym{beta}_j \neq 0.
\end{equation}
Die \textbf{Alternativhypothese} $\sym{H1}$ postuliert hingegen einen signifikanten Effekt ($\symsub{beta}{idx_j} \neq 0$).
Die Entscheidung über die Annahme oder Ablehnung von $\sym{H0}$ basiert auf dem $\sym{p-Wert}$.
Dieser quantifiziert die Wahrscheinlichkeit, unter der Annahme der Gültigkeit von $\sym{H0}$ die beobachteten Daten (oder extremere Ergebnisse) zu erhalten.
Unterschreitet der $\sym{p-Wert}$ das a priori definierte \textbf{Signifikanzniveau} $\sym{alpha}$ (üblicherweise $\sym{alpha} = 0,05$), wird $\sym{H0}$ zugunsten der Alternativhypothese $\sym{H1}$ verworfen.
Der Fehler, $\sym{H0}$ fälschlicherweise abzulehnen, obwohl kein Effekt vorliegt, wird als Fehler 1. Art bezeichnet; seine Wahrscheinlichkeit ist durch $\sym{alpha}$ begrenzt.
Die \textbf{Trennschärfe} fokussiert hingegen auf den Fehler 2. Art (oft mit $\symsub{beta}{idx_error}$ notiert), welcher das Risiko beschreibt, einen tatsächlich vorhandenen Effekt nicht zu erkennen (falsch-negative Entscheidung).
Die Power folglich ist definiert als das Komplement dieses Fehlers ($1 - \symsub{beta}{idx_error}$) und entspricht somit der Wahrscheinlichkeit, $\sym{H0}$ korrekterweise abzulehnen, wenn $\sym{H1}$ wahr ist:
\begin{equation} \label{eq:power}
    \sym{power} = \sym{Pr}(\sym{H0} \text{ ablehnen} \mid \sym{H1} \text{ ist wahr}) = 1 - \symsub{beta}{idx_error}.
\end{equation}
Im Kontext der Lebensdauerdatenanalyse, bei der nicht-normalverteilte und zensierte Daten vorliegen, lassen sich diese Wahrscheinlichkeiten nicht über klassische \textbf{t-Tests} berechnen.
Stattdessen kommen Verfahren für \textbf{Generalisierte Lineare Modelle}, engl. \textbf{\ac{GLM}} zum Einsatz, bei denen die Signifikanz der Koeffizienten $\sym{beta}_j$ vorzugsweise über \textbf{\acp{LR-Test}} (oder approximativ über \textbf{Wald-Tests }- vergleiche hierzu Arbeiten von \textcite{Rigdon.2022,Meeker.2022,Montgomery.2020}) ermittelt wird.
Da für diese komplexen Verteilungsmodelle bei kleinen Stichprobenumfängen $\sym{n}$ keine geschlossenen analytischen Lösungen zur Berechnung der Trennschärfe existieren, erfolgt die Untersuchung gerne numerisch mittels \textbf{Monte-Carlo-Simulation} \cite{Kremer.2021,Arndt.2022,Khuri.2006}.
Hierbei wird der geplante Versuchsplan virtuell vielfach durchlaufen (z.\,B. $\symsub{n}{idx_MC}=10^4$ Simulationsläufe).
Für jeden Lauf werden basierend auf einem angenommenen Modell Ausfallzeiten generiert, verrauscht bzw. mit Streuung überlagert und/oder zensiert.
Anschließend erfolgt die Modellbildung und Hypothesenprüfung.
Der Anteil der Simulationsläufe, in denen der definierte Effekt korrekt als signifikant ($\sym{p-Wert} < \sym{alpha}$) erkannt wird, entspricht der geschätzten $\sym{power}$ des Designs.

\subsection{Statistische Modellbildung} \label{subsec:modellbildung}
Die statistische Modellbildung transformiert die durch den Versuchsplan generierte Datenbasis in einen funktionalen, empirisch basierten Zusammenhang zwischen den Einflussfaktoren $\symsub{x}{idx_i}$ und der Lebensdauerantwort.
Aufgrund der in Abschnitt~\ref{sec:zuv} dargelegten Eigenschaften von Lebensdauerdaten (Nicht-Normalverteilung, Zensierung) sind klassische Regressionsverfahren (\ac{OLS}) hier nicht zulässig.
Stattdessen kommt das Framework der \ac{GLM} zur Anwendung, welches die lineare Prädiktion mit der zugrundeliegenden Verteilung verknüpft \cite{Yang.2007,Myers.2010,Rigdon.2022}.
Hinsichtlich der Modellauswahl greifen klassische Ansätze der Versuchsplanung oft auf Potenztransformationen der Antwortvariablen zurück - wie die \textbf{Box-Cox-Transformation} nach Gleichung~\eqref{eq:box_cox} -, um Varianzhomogenität und Normalverteilung zu approximieren \cite{Box.1964,Montgomery.2021}:
\begin{equation} \label{eq:box_cox}
    \symsub{y}{idx_i}^{(\symsub{eigen_val}{idx_BC})} =
    \begin{cases}
        \frac{\symsub{y}{idx_i}^{\symsub{eigen_val}{idx_BC}} - 1}{\symsub{eigen_val}{idx_BC} \dot{\sym{y}}^{\symsub{eigen_val}{idx_BC}-1}}, & \text{für } \symsub{eigen_val}{idx_BC} \neq 0, \\
        \dot{\sym{y}}\ln(\symsub{y}{idx_i}),                                                                                                & \text{für } \symsub{eigen_val}{idx_BC} = 0.
    \end{cases}
\end{equation}
Im Kontext der expliziten Lebensdaueranalyse ist dieser generische Ansatz jedoch weniger gebräuchlich, da die physikalisch motivierte Verteilungsannahme (z.,B. Weibull) bereits eine inhärente Transformation der Lebensdauerzeit impliziert.
In der Zuverlässigkeitstechnik konkurrieren stattdessen zwei spezifische Modellierungsansätze: das \textbf{Proportional-Hazard-Modell} (PH-Modell, z.B. Cox-Regression) und das \textbf{Log-Location-Scale-Modell}.
Während PH-Modelle insbesondere in der Biostatistik dominieren und ihre Stärke bei der Abbildung zeitabhängiger Kovariaten ausspielen, fokussiert sich die ingenieurwissenschaftliche Praxis - maßgeblich gestützt durch \textcite{Meeker.2022} und \textcite{Yang.2007} - auf Log-Location-Scale-Modelle.
Obwohl für die Weibull-Verteilung eine mathematische Äquivalenz zwischen beiden Ansätzen besteht, sprechen drei fundamentale Gründe für die Verwendung des Log-Location-Scale-Ansatzes:
\begin{enumerate}
    \item \textbf{Physikalischer Bezug:} Gängige physikalische Beschleunigungsmodelle (Arrhenius, Inverses Potenzgesetz) beschreiben die Änderung der \textit{Zeit} bis zum Ausfall ($\ln(\sym{T})$) und nicht der Ausfallrate. Die Log-Linearisierung dieser Gesetze führt direkt auf die Regressionsgleichung der Location-Scale-Modelle.
    \item \textbf{Universalität der Auswertung:} Das Modell bietet einen einheitlichen mathematischen Rahmen, der nicht nur für die Weibull-Verteilung, sondern identisch auch für die Lognormal-Verteilung und Normalverteilung gültig ist. Das PH-Modell hingegen ist für Lognormal-Verteilungen mathematisch nicht geschlossen anwendbar.
    \item \textbf{Fokus auf Lebensdauer-Quantile:} Im Engineering und \ac{DfR} liegt das Interesse primär auf der Prädiktion von Ausfallzeiten für geringe Ausfallanteile (z.,B. $\sym{t}_{10}$). Diese Größen sind direkte Ergebnisse der Location-Scale-Gleichung, während sie im PH-Modell nur indirekt über die Invertierung der Hazard-Funktion zugänglich sind.
\end{enumerate}
Folglich wird die Lebensdauer $\sym{tau}$ als Log-Location-Scale-Regression modelliert.
Hierbei wird der Logarithmus der Lebensdauer $\ln(\sym{tau})$ als Summe einer deterministischen Funktion der Kovariaten $\sym{mu}(\sym{x})$ und eines skalierten Fehlerterms $\sym{epsilon}$ beschrieben:
\begin{equation} \label{eq:log_location_scale}
    \ln(\sym{tau}) = \sym{mu}(\sym{x}) + \sym{sigma} \cdot \sym{epsilon}.
\end{equation}
Der Parameter $\sym{mu}(\sym{x})$ entspricht dem \textbf{Location-Parameter} und bildet die Abhängigkeit der charakteristischen Lebensdauer von den Stressfaktoren $\symsub{x}{idx_j}$ ab (vgl. Gleichung~\ref{eq:linear_std_model}):
\begin{equation} \label{eq:mu_regression}
    \sym{mu}(\sym{x}) = \ln(\sym{T}(\sym{x})) = \symsub{beta}{idx_0} + \sum_{\sym{idx_j}=1}^{\sym{k}} \symsub{beta}{idx_j} \symsub{x}{idx_j} + \sum \sum \sym{beta}_{\sym{idx_j}\sym{idx_l}} \symsub{x}{idx_j} \symsub{x}{idx_l}.
\end{equation}
Der Parameter $\sym{sigma}$ ist der \textbf{Skalenparameter} des Fehlers und beschreibt die Streuung.Im Falle der Weibull-Verteilung korrespondiert dieser direkt mit dem Formparameter durch $\sym{sigma} = 1/\sym{b}$, wobei $\sym{epsilon}$ der Standard-Gumbel-Verteilung (Extremwertverteilung des Typs I) folgt \cite{Meeker.2022}.
Standardmäßig wird $\sym{sigma}$ als konstant über den Versuchsraum angenommen, kann jedoch in erweiterten Modellen ebenfalls als Funktion der Stressfaktoren modelliert werden.
Der Prozess der Modellierung gliedert sich zur Anwendung dieses Frameworks analog zu Abschnitt~\ref{subsec:schätzer} in die methodischen Schritte: \textbf{(1) Modellauswahl}, \textbf{(2) Parameterschätzung} mittels \ac{MLE}, \textbf{(3) Signifikanztest}, \textbf{(4) Varianzschätzung} sowie \textbf{(5) Modellprädiktion} der Zuverlässigkeit mit Aussagesicherheit \cite{Meeker.2022,Rigdon.2022,Nelson.1990}.

% Aufgrund der in Abschnitt~\ref{sec:zuv} dargelegten Eigenschaften von Lebensdauerdaten (Nicht-Normalverteilung, Zensierung) sind klassische Regressionsverfahren (\ac{OLS}) wie schon geschildert hier nicht zulässig.
% Stattdessen kann das Framework der \ac{GLM} zur Anwendung kommen, welches die lineare Prädiktion mit der zugrundeliegenden Weibull-Verteilung verknüpft: \textbf{Weibull-\ac{GLM}} \cite{Yang.2007,Myers.2010,Rigdon.2022}.
% Der Prozess der Modellierung gliedert sich hierbei analog zu Abschnitt~\ref{subsec:schätzer} in die  methodischen Schritte: \textbf{(1) Modellauswahl}, \textbf{(2) Parameterschätzung}, \textbf{(3) Signifikanztest}, \textbf{(4) Varianzschätzung und Unsicherheit} sowie \textbf{(5) Modellprädiktion und Zuverlässigkeitsschätzung mit Aussagesicherheit} \cite{Meeker.2022,Rigdon.2022,Nelson.1990}.

% Wird also für die Lebensdauerverteilung die Weibull-Verteilung (Abschnitt~\ref{subsec:paramleben}) angenommen, so ergibt sich für $\sym{idx_j}=0,...,\sym{k}$ betrachtete Parameter aus beispielsweise Gleichung~\ref{eq:quadric_std_model} und $\sym{n}$ Beobachtungen analog zu Gleichung~\ref{eq:weibull_pdf} die charakteristische Lebensdauer als Funktion der Einflussfaktoren $\symsub{x}{idx_i}$ und des unbekannten Koeffizientenvektors $\hat{\sym{Beta}}$:
% \begin{equation} \label{eq:glm_link}
%     \ln(\sym{T}) = \hat{\sym{y}} = \symsub{beta}{idx_0} + \sum_{\sym{idx_j}=1}^{\sym{k}} \symsub{beta}{idx_j} \symsub{x}{idx_j} + \dots + \sym{epsilon}.
% \end{equation}
% Hinsichtlich der \textbf{Modellauswahl} ist dabei im Vorfeld zu prüfen, ob die funktionale Beziehung zwischen $\sym{y}$ und $\sym{x}$ linearisiert werden muss.
% Klassische Ansätze der Versuchsplanung greifen hierzu oft auf Potenztransformationen der Antwortvariablen, wie die \textbf{Box-Cox-Transformation}, zurück, um Varianzhomogenität und Normalverteilung zu approximieren, vgl. die Arbeiten von \textcite{Box.1964,Montgomery.2021}.
% Als Teil der Modellparameter $\sym{theta}$ wird dabei der Transformationsparameter $\symsub{eigen_val}{idx_BC}$ geschätzt, welcher die optimale Transformation der Antwortvariable spezifiziert.
% So wird die Zielvariable durch eine Potenztransformation der Form $\sym{y}^{(\symsub{eigen_val}{idx_BC})}$ oder $\ln(\sym{y})$ (für $\symsub{eigen_val}{idx_BC} = 0$ mit geometrischem Mittelwert der Beobachtung $\dot{\sym{y}} = \exp \left[ \frac{1}{\sym{n}} \sum_{\sym{idx_i}=1}^{\sym{n}} \ln(\symsub{y}{idx_i}) \right]$) transformiert, um die Modellannahmen bestmöglich zu erfüllen:
% \begin{equation} \label{eq:box_cox}
%     \symsub{y}{idx_i}^{(\symsub{eigen_val}{idx_BC})} =
%     \begin{cases}
%         \frac{\symsub{y}{idx_i}^{\symsub{eigen_val}{idx_BC}} - 1}{\symsub{eigen_val}{idx_BC} \dot{\sym{y}}^{\symsub{eigen_val}{idx_BC}-1}}, & \text{für } \symsub{eigen_val}{idx_BC} \neq 0, \\
%         \dot{\sym{y}}\ln(\symsub{y}{idx_i}),                                                                                                & \text{für } \symsub{eigen_val}{idx_BC} = 0.
%     \end{cases}
% \end{equation}
% Im Kontext der expliziten Lebensdauer-Datenanalyse ist dieser Ansatz jedoch weniger gebräuchlich, da die Verteilungsannahme (Weibull, Lognormal, etc.) bereits eine inhärente Transformation der Lebensdauerzeit impliziert.
% In der Zuverlässigkeitstechnik konkurriert dieser parametrische Ansatz daher unter anderem mit \textbf{Proportional-Hazard-Modellen} (PH-Modelle, z.\,B. via Cox-Regression nach \textcite{Cox.1972}), welche die multiplikative Wirkung von Kovariaten auf die Ausfallrate $\sym{lambda}$ statt auf die Lebensdauerzeit modellieren \cite{Meeker.2022}.
% Die Weibulldichte mit Formparameter $\sym{b}$ und Skalenparameter $\sym{T}$ nach Gleichung~\ref{eq:glm_link} ergibt sich dabei zu:
% \begin{equation} \label{eq:weibull_pdf}
%     \sym{f}(\sym{t},\sym{X}) = \sym{eigen_val}(\sym{t}, \sym{X}) \cdot \sym{R}(\sym{t}, \sym{X}) = \sym{b} \cdot \sym{t}^{\sym{b}-1} \cdot e^{\left[ \sym{y} - \sym{t}^{\sym{b}} e^{\sym{y}} \right]}
% \end{equation}





Das hier fokussierte \textbf{Weibull-\ac{GLM}} nimmt eine Sonderstellung ein: Es lässt sich sowohl als PH-Modell als auch als \textbf{Accelerated Failure Time}-Modell (AFT) interpretieren.
Da im Engineering die Prädiktion konkreter Ausfallzeiten (Quantile) im Vordergrund steht, wird der AFT-Ansatz gemäß Gleichung~\ref{eq:glm_link} favorisiert, bei dem die Kovariaten die Zeitskala der Lebensdauerverteilung direkt stauchen oder strecken \cite{Rigdon.2022,Nelson.1990}.


\textbf{1. Modellauswahl (Weibull-GLM):}
Das Kernstück bildet die Verknüpfung der linearen Prädiktorfunktion $\eta = \sym{x}'\sym{Beta}$ mit dem charakteristischen Lebensdauerparameter $\sym{T}$ über eine logarithmische Link-Funktion.
Das \textbf{Weibull-GLM} (auch Cox-Weibull-Modell) modelliert die Abhängigkeit der Lebensdauer $\sym{T}(\sym{x})$ von den Stressfaktoren (z.\,B. Temperatur, Last) üblicherweise als Log-Linear-Beziehung (Arrhenius- oder Inverse-Power-Law-Ansatz):

Der Formparameter $\sym{b}$ wird hierbei häufig als konstant über den Versuchsraum angenommen (konstante Ausfallmechanik), kann jedoch in erweiterten Modellen ebenfalls als funktionale Abhängigkeit modelliert werden \cite{Meeker.2022}.

\textbf{2. Parameterschätzung:}
Die Bestimmung des unbekannten Koeffizientenvektors $\hat{\sym{Beta}}$ erfolgt mittels \textbf{\ac{MLE}} auf Basis der in Abschnitt~\ref{subsubsec:mle} definierten Log-Likelihood-Funktion $\sym{Lambda}$ für zensierte Daten.
Da für dieses Gleichungssystem keine geschlossene analytische Lösung existiert, werden iterative numerische Optimierungsverfahren eingesetzt.
Neben klassischen Gradientenverfahren (Newton-Raphson) haben sich hier insbesondere direkte Suchverfahren wie \textbf{Pattern Search} als robust erwiesen, um das globale Maximum der Likelihood-Funktion auch bei flachen Wirkungsflächen zuverlässig zu identifizieren \cite{Qiao.1994}.

\textbf{3. Signifikanzprüfung:}
Zur Selektion der relevanten Modellterme (Haupteffekte, Wechselwirkungen) wird der \textbf{Likelihood-Ratio-Test} (\ac{LR-Test}) herangezogen.
Er vergleicht die Güte eines vollständigen Modells mit der eines reduzierten Modells, indem die Differenz ihrer Log-Likelihood-Werte (die \textit{Devianz}) bewertet wird.
Der \ac{LR-Test} gilt bei kleinen bis mittleren Stichprobenumfängen, wie sie in der Lebensdauererprobung üblich sind, gegenüber dem approximativen Wald-Test als überlegen und präziser hinsichtlich der Einhaltung des Signifikanzniveaus $\sym{alpha}$ \cite{Rigdon.2022,Nelson.1990}.

\textbf{4. Varianzschätzung und Unsicherheit:}
Die Präzision der geschätzten Koeffizienten wird über die \textbf{Fisher-Informationsmatrix} quantifiziert.
Die Invertierung der beobachteten Fisher-Information $\symsub{FIM}{idx_O}$ an der Stelle der \ac{MLE}-Lösung liefert die asymptotische Varianz-Kovarianz-Matrix $\hat{\sym{V}}$ (vgl. Gleichung~\ref{eq:var_covar}).
Daraus leiten sich die Standardfehler und die \textit{Wald}-Vertrauensbereiche (Fisher-CIs) für die Regressionskoeffizienten ab, welche die Grundlage für die Berechnung der Unsicherheitsbänder in der Prädiktion bilden.

\textbf{5. Modellprädiktion und Zuverlässigkeit:}
Das final parametrisierte Modell erlaubt die \textbf{Prädiktion} der charakteristischen Lebensdauer $\hat{\sym{y}} = \hat{\sym{T}}(\sym{x}_0)$ an jedem beliebigen Punkt $\sym{x}_0$ des Designraums (Inter- und Extrapolation).
Durch Einsetzen in die Weibull-Verteilungsfunktion (Gleichung~\ref{eq:weibull_cdf}) resultiert die geschätzte \textbf{Zuverlässigkeitsfunktion} $\hat{\sym{R}}(\sym{t}|\sym{x}_0)$.
Die zugehörigen Vertrauensbereiche für $\hat{\sym{R}}$ oder spezifische Quantile $\hat{\sym{t}}_q$ werden unter Berücksichtigung der Kovarianzen in $\hat{\sym{V}}$ mittels Delta-Methode approximiert, um die statistische Sicherheit der Aussage für den Anwender transparent auszuweisen.



\begin{itemize}
    \item \colorbox{yellow}{\cite[Chapter. 27]{Cui.2021}}
    \item \colorbox{yellow}{GLL-Weibull}
    \item \colorbox{yellow}{LR-Ratio}
    \item \colorbox{yellow}{Varianzen}
    \item \colorbox{yellow}{Modellaufbau}
    \item Extrapolation: bedarf in der Praxis / oder sind doch dann mal VPs verfügbar und das modell wird einfach erweitert

    \item Risiduen (Cox-Snell \cite{Rigdon.2022})
\end{itemize}
\begin{itemize}
    \item \colorbox{yellow}{wu}
    \item \colorbox{yellow}{wütherich}
    \item \colorbox{yellow}{yang p282 erster abschnitt}
    \item \colorbox{yellow}{russell doe for glm kap1.4 p12}
\end{itemize}


\cite{RisbergEllekjr.1998,Bisgaard.2011,Zahran.2003,GiovannittiJensen.1989,Hinkelmann.2012,Khuri.2006,Johnson.2011,Donev.2004,G.E.P.Box.1951,Ardakani.2011,Jones.2012,Jones.2021,Wu.2021,Khuri.2006,Escobar.1995,Modarres.2017,Ahn.2015,Rasch.2018,Xu.2002,Wald.1943,Rencher.2008,Box.2007,Fisher.1935,Myers.2010,Box.1988}